{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introducción al Mundo de Data Science y Machine Learning\n",
    "\n",
    "## 0.1. Un Viaje a Través del Tiempo\n",
    "\n",
    "### 0.1.1. Los Orígenes\n",
    "   #### 0.1.1.1. De la estadística a la inteligencia artificial\n",
    "      - El censo de 1880 y la máquina tabuladora de Hollerith\n",
    "      - Alan Turing y la máquina de Turing\n",
    "   #### 0.1.1.2. El nacimiento del Machine Learning\n",
    "      - Arthur Samuel y el juego de damas (1959)\n",
    "      - El perceptrón de Frank Rosenblatt (1958)\n",
    "\n",
    "### 0.1.2. La Revolución de los Datos\n",
    "   #### 0.1.2.1. La era del Big Data\n",
    "      - De megabytes a zettabytes: el crecimiento exponencial de los datos\n",
    "      - El papel de internet y las redes sociales\n",
    "   #### 0.1.2.2. El renacimiento del Deep Learning\n",
    "      - ImageNet y el punto de inflexión de 2012\n",
    "      - GPT y la revolución del procesamiento del lenguaje natural\n",
    "\n",
    "## 0.2. Matemáticas: El Lenguaje Secreto de los Datos\n",
    "\n",
    "### 0.2.1. Álgebra Lineal: El Esqueleto de los Algoritmos\n",
    "   #### 0.2.1.1. Matrices: Más allá de filas y columnas\n",
    "      - Cómo Netflix usa álgebra lineal para recomendar películas\n",
    "      - PageRank: El álgebra detrás del motor de búsqueda de Google\n",
    "   #### 0.2.1.2. Vectores: Dando dirección a los datos\n",
    "      - Word embeddings: Representando palabras como vectores\n",
    "      - El truco del kernel: Transformando datos no lineales\n",
    "\n",
    "### 0.2.2. Cálculo: El Motor del Aprendizaje\n",
    "   #### 0.2.2.1. Derivadas: Encontrando el camino óptimo\n",
    "      - Gradiente descendente: Cómo las redes neuronales \"aprenden\"\n",
    "      - La paradoja de la ballena azul y la optimización\n",
    "   #### 0.2.2.2. Integrales: Sumando infinitos pequeños\n",
    "      - Monte Carlo: Integrando con dados y adivinando π\n",
    "      - Transformada de Fourier: De señales de audio a espectrogramas\n",
    "\n",
    "## 0.3. Datos Curiosos y Aplicaciones Sorprendentes\n",
    "\n",
    "### 0.3.1. Machine Learning en la Vida Cotidiana\n",
    "   #### 0.3.1.1. Tecnología que parece magia\n",
    "      - Filtros de Snapchat: Geometría computacional en tu cara\n",
    "      - Siri y Alexa: Asistentes virtuales que entienden (casi) todo\n",
    "   #### 0.3.1.2. ML donde menos te lo esperas\n",
    "      - Tinder: Algoritmos de matching para encontrar el amor\n",
    "      - Spotify: Descubrimiento semanal y la personalización musical\n",
    "\n",
    "### 0.3.2. Data Science Salvando el Mundo\n",
    "   #### 0.3.2.1. Medicina y salud\n",
    "      - Detección temprana de cáncer con Deep Learning\n",
    "      - Predicción de pandemias: De Google Flu Trends a COVID-19\n",
    "   #### 0.3.2.2. Medio ambiente y sostenibilidad\n",
    "      - Monitoreo de deforestación con imágenes satelitales\n",
    "      - Optimización de rutas para reducir emisiones de CO2\n",
    "\n",
    "## 0.4. Desafíos y Juegos para Despertar tu Intuición\n",
    "\n",
    "### 0.4.1. Rompecabezas Estadísticos\n",
    "   #### 0.4.1.1. La paradoja de Simpson\n",
    "      - Cuando agrupar datos lleva a conclusiones opuestas\n",
    "   #### 0.4.2.2. El problema del cumpleaños\n",
    "      - Probabilidades contraintuitivas en un salón de clases\n",
    "\n",
    "### 0.4.2. Experimentos Interactivos\n",
    "   #### 0.4.2.1. Construye tu propio clasificador\n",
    "      - Juego: Enseña a una IA a distinguir perros de gatos\n",
    "   #### 0.4.2.2. Visualización de algoritmos\n",
    "      - Explora cómo aprende una red neuronal en tiempo real\n",
    "\n",
    "## 0.5. El Futuro: Horizontes Inexplorados\n",
    "\n",
    "### 0.5.1. Inteligencia Artificial General\n",
    "   #### 0.5.1.1. El test de Turing y más allá\n",
    "      - ¿Podrán las máquinas realmente pensar?\n",
    "   #### 0.5.1.2. Ética y responsabilidad\n",
    "      - El dilema del tranvía: Decisiones éticas para coches autónomos\n",
    "\n",
    "### 0.5.2. Computación Cuántica y Machine Learning\n",
    "   #### 0.5.2.1. Qubits y superposición\n",
    "      - Cómo la física cuántica podría revolucionar el ML\n",
    "   #### 0.5.2.2. Algoritmos cuánticos\n",
    "      - Resolviendo problemas \"imposibles\" en segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fundamentos Matemáticos para Data Science y Machine Learning\n",
    "\n",
    "## 1.1. Álgebra Lineal\n",
    "### 1.1.1. Vectores y Matrices\n",
    "#### 1.1.1.1. Operaciones básicas con vectores\n",
    "##### 1.1.1.1.1. Suma y resta de vectores\n",
    "##### 1.1.1.1.2. Producto escalar y vectorial\n",
    "#### 1.1.1.2. Operaciones básicas con matrices\n",
    "##### 1.1.1.2.1. Suma y resta de matrices\n",
    "##### 1.1.1.2.2. Multiplicación de matrices\n",
    "### 1.1.2. Sistemas de ecuaciones lineales\n",
    "#### 1.1.2.1. Método de eliminación de Gauss-Jordan\n",
    "##### 1.1.2.1.1. Pasos del algoritmo\n",
    "##### 1.1.2.1.2. Implementación en Python\n",
    "#### 1.1.2.2. Aplicaciones en Data Science\n",
    "##### 1.1.2.2.1. Resolución de sistemas en regresión lineal\n",
    "##### 1.1.2.2.2. Inversión de matrices en análisis de datos\n",
    "\n",
    "## 1.2. Cálculo\n",
    "### 1.2.1. Derivadas\n",
    "#### 1.2.1.1. Reglas de derivación\n",
    "##### 1.2.1.1.1. Regla de la cadena\n",
    "##### 1.2.1.1.2. Derivadas parciales\n",
    "#### 1.2.1.2. Optimización y gradiente descendente\n",
    "##### 1.2.1.2.1. Método de Newton\n",
    "##### 1.2.1.2.2. Implementación de gradiente descendente en Python\n",
    "### 1.2.2. Integrales\n",
    "#### 1.2.2.1. Técnicas de integración\n",
    "##### 1.2.2.1.1. Integración por partes\n",
    "##### 1.2.2.1.2. Integración numérica (regla del trapecio, Simpson)\n",
    "#### 1.2.2.2. Aplicaciones en probabilidad y estadística\n",
    "##### 1.2.2.2.1. Cálculo de áreas bajo la curva de densidad\n",
    "##### 1.2.2.2.2. Momentos de distribuciones continuas\n",
    "\n",
    "## 1.3. Probabilidad\n",
    "### 1.3.1. Conceptos básicos\n",
    "#### 1.3.1.1. Espacios muestrales y eventos\n",
    "##### 1.3.1.1.1. Diagramas de Venn y operaciones de conjuntos\n",
    "##### 1.3.1.1.2. Probabilidad de la unión e intersección de eventos\n",
    "#### 1.3.1.2. Probabilidad condicional y Teorema de Bayes\n",
    "##### 1.3.1.2.1. Independencia de eventos\n",
    "##### 1.3.1.2.2. Aplicaciones del Teorema de Bayes en Machine Learning\n",
    "### 1.3.2. Variables aleatorias\n",
    "#### 1.3.2.1. Distribuciones discretas y continuas\n",
    "##### 1.3.2.1.1. Bernoulli, Binomial y Poisson\n",
    "##### 1.3.2.1.2. Normal, Exponencial y t-Student\n",
    "#### 1.3.2.2. Esperanza y varianza\n",
    "##### 1.3.2.2.1. Propiedades de la esperanza y varianza\n",
    "##### 1.3.2.2.2. Covarianza y correlación\n",
    "\n",
    "# 2. Estadística para Data Science\n",
    "\n",
    "## 2.1. Estadística Descriptiva\n",
    "### 2.1.1. Medidas de tendencia central\n",
    "#### 2.1.1.1. Media, mediana y moda\n",
    "##### 2.1.1.1.1. Propiedades y comparación\n",
    "##### 2.1.1.1.2. Casos de uso en análisis de datos\n",
    "#### 2.1.1.2. Implementación en Python\n",
    "##### 2.1.1.2.1. Uso de NumPy y Pandas\n",
    "##### 2.1.1.2.2. Visualización con Matplotlib y Seaborn\n",
    "### 2.1.2. Medidas de dispersión\n",
    "#### 2.1.2.1. Varianza, desviación estándar y rango intercuartílico\n",
    "##### 2.1.2.1.1. Cálculo e interpretación\n",
    "##### 2.1.2.1.2. Coeficiente de variación\n",
    "#### 2.1.2.2. Visualización con Python\n",
    "##### 2.1.2.2.1. Boxplots y violinplots\n",
    "##### 2.1.2.2.2. Histogramas y densidad kernel\n",
    "\n",
    "## 2.2. Inferencia Estadística\n",
    "### 2.2.1. Estimación\n",
    "#### 2.2.1.1. Estimadores puntuales y por intervalos\n",
    "##### 2.2.1.1.1. Propiedades de los estimadores (sesgo, consistencia)\n",
    "##### 2.2.1.1.2. Intervalos de confianza\n",
    "#### 2.2.1.2. Métodos de máxima verosimilitud\n",
    "##### 2.2.1.2.1. Función de verosimilitud\n",
    "##### 2.2.1.2.2. Estimación de parámetros con Python\n",
    "### 2.2.2. Pruebas de hipótesis\n",
    "#### 2.2.2.1. Pruebas paramétricas y no paramétricas\n",
    "##### 2.2.2.1.1. t-test y ANOVA\n",
    "##### 2.2.2.1.2. Chi-cuadrado y prueba de Wilcoxon\n",
    "#### 2.2.2.2. Implementación de pruebas en Python\n",
    "##### 2.2.2.2.1. Uso de scipy.stats\n",
    "##### 2.2.2.2.2. Interpretación y visualización de resultados\n",
    "\n",
    "## 2.3. Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "### 2.3.1. Introducción al EDA\n",
    "#### 2.3.1.1. Objetivos del EDA\n",
    "##### 2.3.1.1.1. Comprender la estructura de los datos\n",
    "##### 2.3.1.1.2. Identificación de patrones y tendencias\n",
    "#### 2.3.1.2. Técnicas y herramientas para el EDA\n",
    "##### 2.3.1.2.1. Herramientas estadísticas básicas\n",
    "##### 2.3.1.2.2. Visualización y resumen de datos\n",
    "\n",
    "### 2.3.2. Análisis Univariado\n",
    "#### 2.3.2.1. Distribución de una variable\n",
    "##### 2.3.2.1.1. Histogramas y distribuciones de frecuencia\n",
    "##### 2.3.2.1.2. Análisis de outliers y valores atípicos\n",
    "#### 2.3.2.2. Transformaciones y estandarización\n",
    "##### 2.3.2.2.1. Logaritmos y escalado\n",
    "##### 2.3.2.2.2. Estandarización (Z-score)\n",
    "\n",
    "### 2.3.3. Análisis Bivariado\n",
    "#### 2.3.3.1. Relación entre dos variables\n",
    "##### 2.3.3.1.1. Tablas de contingencia y correlación\n",
    "##### 2.3.3.1.2. Gráficos de dispersión y análisis de covarianza\n",
    "#### 2.3.3.2. Pruebas estadísticas para correlación\n",
    "##### 2.3.3.2.1. Coeficiente de correlación de Pearson y Spearman\n",
    "##### 2.3.3.2.2. Implementación y visualización en Python\n",
    "\n",
    "### 2.3.4. Análisis Multivariado\n",
    "#### 2.3.4.1. Matrices de correlación y análisis de patrones\n",
    "##### 2.3.4.1.1. Mapas de calor y análisis de clusters\n",
    "##### 2.3.4.1.2. Identificación de relaciones no lineales\n",
    "#### 2.3.4.2. Reducción de dimensionalidad\n",
    "##### 2.3.4.2.1. Análisis de componentes principales (PCA)\n",
    "##### 2.3.4.2.2. Implementación de PCA en Python\n",
    "\n",
    "### 2.3.5. Detección y Tratamiento de Valores Atípicos\n",
    "#### 2.3.5.1. Identificación de outliers\n",
    "##### 2.3.5.1.1. Métodos basados en desviación estándar y percentiles\n",
    "##### 2.3.5.1.2. Visualización de outliers con boxplots\n",
    "#### 2.3.5.2. Estrategias para el tratamiento de outliers\n",
    "##### 2.3.5.2.1. Eliminación vs. imputación de valores\n",
    "##### 2.3.5.2.2. Técnicas avanzadas de manejo de outliers\n",
    "\n",
    "### 2.3.6. Tratamiento de Datos Faltantes\n",
    "#### 2.3.6.1. Identificación de datos faltantes\n",
    "##### 2.3.6.1.1. Métodos gráficos y tablas de resumen\n",
    "##### 2.3.6.1.2. Técnicas de análisis de patrones de faltantes\n",
    "#### 2.3.6.2. Imputación de datos faltantes\n",
    "##### 2.3.6.2.1. Imputación simple (media, mediana, moda)\n",
    "##### 2.3.6.2.2. Métodos avanzados de imputación (KNN, MICE)\n",
    "#### 2.3.6.3. Implementación en Python\n",
    "##### 2.3.6.3.1. Uso de scikit-learn y pandas para imputación\n",
    "##### 2.3.6.3.2. Evaluación de impacto en el análisis posterior\n",
    "\n",
    "# 3. Aprendizaje Supervisado\n",
    "\n",
    "## 3.1. Regresión\n",
    "### 3.1.1. Regresión lineal simple y múltiple\n",
    "#### 3.1.1.1. Mínimos cuadrados ordinarios\n",
    "##### 3.1.1.1.1. Derivación de la fórmula de mínimos cuadrados\n",
    "##### 3.1.1.1.2. Supuestos del modelo lineal\n",
    "#### 3.1.1.2. Implementación y visualización en Python\n",
    "##### 3.1.1.2.1. Uso de scikit-learn para regresión\n",
    "##### 3.1.1.2.2. Diagnóstico del modelo (residuos, Q-Q plots)\n",
    "### 3.1.2. Regresión polinomial y regularización\n",
    "#### 3.1.2.1. Ridge y Lasso\n",
    "##### 3.1.2.1.1. Diferencias entre L1 y L2 regularización\n",
    "##### 3.1.2.1.2. Selección del parámetro de regularización\n",
    "#### 3.1.2.2. Validación cruzada para selección de modelos\n",
    "##### 3.1.2.2.1. K-fold cross-validation\n",
    "##### 3.1.2.2.2. Implementación de validación cruzada en Python\n",
    "\n",
    "## 3.2. Clasificación\n",
    "### 3.2.1. Regresión logística\n",
    "#### 3.2.1.1. Función sigmoide y máxima verosimilitud\n",
    "##### 3.2.1.1.1. Interpretación de coeficientes\n",
    "##### 3.2.1.1.2. Odds ratio y probabilidades\n",
    "#### 3.2.1.2. Implementación y evaluación en Python\n",
    "##### 3.2.1.2.1. Uso de scikit-learn para clasificación\n",
    "##### 3.2.1.2.2. Métricas de evaluación (precisión, recall, F1-score)\n",
    "### 3.2.2. Árboles de decisión y bosques aleatorios\n",
    "#### 3.2.2.1. Criterios de división y poda\n",
    "##### 3.2.2.1.1. Ganancia de información y índice Gini\n",
    "##### 3.2.2.1.2. Técnicas de poda para evitar sobreajuste\n",
    "#### 3.2.2.2. Bagging y boosting\n",
    "##### 3.2.2.2.1. Random Forests y su implementación\n",
    "##### 3.2.2.2.2. Gradient Boosting y XGBoost\n",
    "\n",
    "# 4. Aprendizaje No Supervisado\n",
    "\n",
    "## 4.1. Clustering\n",
    "### 4.1.1. K-means\n",
    "#### 4.1.1.1. Algoritmo y convergencia\n",
    "##### 4.1.1.1.1. Inicialización de centroides (método de Forgy, k-means++)\n",
    "##### 4.1.1.1.2. Complejidad computacional y optimizaciones\n",
    "#### 4.1.1.2. Implementación y visualización en Python\n",
    "##### 4.1.1.2.1. Uso de scikit-learn para K-means\n",
    "##### 4.1.1.2.2. Elbow method para selección de K\n",
    "### 4.1.2. Clustering jerárquico\n",
    "#### 4.1.2.1. Métodos aglomerativos y divisivos\n",
    "##### 4.1.2.1.1. Métricas de distancia (euclidiana, Manhattan, coseno)\n",
    "##### 4.1.2.1.2. Criterios de linkage (single, complete, average)\n",
    "#### 4.1.2.2. Dendrogramas y selección de clusters\n",
    "##### 4.1.2.2.1. Interpretación de dendrogramas\n",
    "##### 4.1.2.2.2. Implementación en Python con scipy\n",
    "\n",
    "## 4.2. Reducción de dimensionalidad\n",
    "### 4.2.1. Análisis de Componentes Principales (PCA)\n",
    "#### 4.2.1.1. Descomposición en valores singulares\n",
    "##### 4.2.1.1.1. Cálculo de autovalores y autovectores\n",
    "##### 4.2.1.1.2. Varianza explicada y selección de componentes\n",
    "#### 4.2.1.2. Aplicación en compresión de imágenes\n",
    "##### 4.2.1.2.1. Implementación de PCA con NumPy\n",
    "##### 4.2.1.2.2. Visualización de componentes principales\n",
    "### 4.2.2. t-SNE y UMAP\n",
    "#### 4.2.2.1. Fundamentos matemáticos\n",
    "##### 4.2.2.1.1. Divergencia de Kullback-Leibler\n",
    "##### 4.2.2.1.2. Optimización de embebimientos\n",
    "#### 4.2.2.2. Visualización de datos de alta dimensionalidad\n",
    "##### 4.2.2.2.1. Implementación con scikit-learn y umap-learn\n",
    "##### 4.2.2.2.2. Comparación de resultados entre t-SNE y UMAP\n",
    "\n",
    "# 5. Redes Neuronales y Deep Learning\n",
    "\n",
    "## 5.1. Perceptrón y redes feed-forward\n",
    "### 5.1.1. Funciones de activación\n",
    "#### 5.1.1.1. Sigmoid, ReLU y sus variantes\n",
    "##### 5.1.1.1.1. Propiedades y derivadas de las funciones de activación\n",
    "##### 5.1.1.1.2. Impacto en el aprendizaje y vanishing gradient\n",
    "#### 5.1.1.2. Implementación de una red neuronal simple\n",
    "##### 5.1.1.2.1. Forward propagation\n",
    "##### 5.1.1.2.2. Inicialización de pesos (Xavier/Glorot, He)\n",
    "### 5.1.2. Backpropagation\n",
    "#### 5.1.2.1. Regla de la cadena y gradientes\n",
    "##### 5.1.2.1.1. Derivación de las ecuaciones de backpropagation\n",
    "##### 5.1.2.1.2. Cálculo eficiente de gradientes\n",
    "#### 5.1.2.2. Optimizadores: SGD, Adam, RMSprop\n",
    "##### 5.1.2.2.1. Momentum y adaptative learning rates\n",
    "##### 5.1.2.2.2. Implementación de optimizadores en Python\n",
    "\n",
    "## 5.2. Arquitecturas avanzadas\n",
    "### 5.2.1. Redes convolucionales (CNN)\n",
    "#### 5.2.1.1. Convolución y pooling\n",
    "##### 5.2.1.1.1. Tipos de capas convolucionales (1D, 2D, 3D)\n",
    "##### 5.2.1.1.2. Métodos de pooling (max, average, global)\n",
    "#### 5.2.1.2. Transferencia de aprendizaje con modelos pre-entrenados\n",
    "##### 5.2.1.2.1. Fine-tuning de redes como VGG, ResNet, Inception\n",
    "##### 5.2.1.2.2. Implementación con Keras/TensorFlow\n",
    "### 5.2.2. Redes recurrentes (RNN) y LSTM\n",
    "#### 5.2.2.1. Procesamiento de secuencias\n",
    "##### 5.2.2.1.1. Arquitectura de celdas LSTM y GRU\n",
    "##### 5.2.2.1.2. Bidirectional RNNs\n",
    "#### 5.2.2.2. Aplicaciones en procesamiento de lenguaje natural\n",
    "##### 5.2.2.2.1. Modelo seq2seq para traducción automática\n",
    "##### 5.2.2.2.2. Implementación de un modelo de generación de texto\n",
    "\n",
    "# 6. Técnicas Avanzadas y Aplicaciones\n",
    "\n",
    "## 6.1. Procesamiento de Lenguaje Natural\n",
    "### 6.1.1. Modelos de espacio vectorial\n",
    "#### 6.1.1.1. TF-IDF y word embeddings\n",
    "##### 6.1.1.1.1. Cálculo e interpretación de TF-IDF\n",
    "##### 6.1.1.1.2. Word2Vec, GloVe y FastText\n",
    "#### 6.1.1.2. Implementación de un sistema de recomendación\n",
    "##### 6.1.1.2.1. Similitud coseno para recomendaciones\n",
    "##### 6.1.1.2.2. Evaluación de sistemas de recomendación\n",
    "### 6.1.2. Modelos de lenguaje\n",
    "#### 6.1.2.1. N-gramas y modelos basados en transformers\n",
    "##### 6.1.2.1.1. Suavizado y backoff en modelos n-grama\n",
    "##### 6.1.2.1.2. Arquitectura del transformer (attention mechanisms)\n",
    "#### 6.1.2.2. Fine-tuning de modelos pre-entrenados\n",
    "##### 6.1.2.2.1. BERT, GPT y sus variantes\n",
    "##### 6.1.2.2.2. Implementación de fine-tuning con Hugging Face\n",
    "\n",
    "## 6.2. Visión por Computadora\n",
    "### 6.2.1. Procesamiento de imágenes\n",
    "#### 6.2.1.1. Filtros y detección de bordes\n",
    "##### 6.2.1.1.1. Convolución 2D y filtros (Sobel, Laplacian)\n",
    "##### 6.2.1.1.2. Implementación con OpenCV y scikit-image\n",
    "#### 6.2.1.2. Segmentación de imágenes con U-Net\n",
    "##### 6.2.1.2.1. Arquitectura U-Net y sus variantes\n",
    "##### 6.2.1.2.2. Entrenamiento y evaluación de U-Net en Python\n",
    "### 6.2.2. Detección y reconocimiento de objetos\n",
    "#### 6.2.2.1. YOLO y R-CNN\n",
    "##### 6.2.2.1.1. Arquitectura y funcionamiento de YOLO\n",
    "##### 6.2.2.1.2. Fast R-CNN y Faster R-CNN\n",
    "#### 6.2.2.2. Implementación de un sistema de detección de objetos\n",
    "##### 6.2.2.2.1. Uso de YOLOv5 con PyTorch\n",
    "##### 6.2.2.2.2. Evaluación de modelos (mAP, IoU)\n",
    "\n",
    "# 7. Ética y Responsabilidad en Data Science y Machine Learning\n",
    "\n",
    "## 7.1. Sesgos y fairness\n",
    "### 7.1.1. Tipos de sesgos en datos y modelos\n",
    "#### 7.1.1.1. Métricas de fairness\n",
    "##### 7.1.1.1.1. Paridad demográfica\n",
    "##### 7.1.1.1.2. Igualdad de oportunidades\n",
    "#### 7.1.1.2. Técnicas de mitigación de sesgos\n",
    "##### 7.1.1.2.1. Preprocesamiento de datos\n",
    "##### 7.1.1.2.2. Ajuste de modelos para equidad\n",
    "### 7.1.2. Interpretabilidad y explicabilidad\n",
    "#### 7.1.2.1. LIME y SHAP\n",
    "##### 7.1.2.1.1. Funcionamiento de LIME\n",
    "##### 7.1.2.1.2. Valores Shapley y su interpretación\n",
    "#### 7.1.2.2. Implementación de explicaciones para modelos de caja negra\n",
    "##### 7.1.2.2.1. Uso de bibliotecas de explicabilidad en Python\n",
    "##### 7.1.2.2.2. Visualización de explicaciones de modelos\n",
    "\n",
    "## 7.2. Privacidad y seguridad\n",
    "### 7.2.1. Anonimización de datos\n",
    "#### 7.2.1.1. K-anonimidad y differential privacy\n",
    "##### 7.2.1.1.1. Conceptos básicos de k-anonimidad\n",
    "##### 7.2.1.2.2. Implementación de differential privacy\n",
    "#### 7.2.1.2. Implementación de técnicas de privacidad en Python\n",
    "##### 7.2.1.2.1. Uso de bibliotecas de privacidad\n",
    "##### 7.2.1.2.2. Evaluación del impacto en la utilidad de los datos\n",
    "### 7.2.2. Ataques adversarios\n",
    "#### 7.2.2.1. Ejemplos adversarios y defensa\n",
    "##### 7.2.2.1.1. Tipos de ataques adversarios\n",
    "##### 7.2.2.1.2. Técnicas de defensa contra ataques\n",
    "#### 7.2.2.2. Robustez de modelos de Machine Learning\n",
    "##### 7.2.2.2.1. Evaluación de la robustez de modelos\n",
    "##### 7.2.2.2.2. Entrenamiento adversario para mejorar la robustez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
