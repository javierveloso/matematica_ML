{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fundamentos Matemáticos para Data Science y Machine Learning\n",
    "\n",
    "La matemática es el pilar fundamental sobre el que se construyen los conceptos de Data Science y Machine Learning. Comprender los fundamentos matemáticos te permitirá entender mejor los algoritmos y técnicas que aplicas. En esta sección, exploraremos el álgebra lineal, empezando por vectores y matrices.\n",
    "\n",
    "## 1.1. Álgebra Lineal\n",
    "\n",
    "El álgebra lineal es esencial para el análisis y la manipulación de datos en múltiples dimensiones. Permite representar y resolver sistemas de ecuaciones lineales, trabajar con transformaciones lineales y comprender las estructuras de datos en espacios vectoriales.\n",
    "\n",
    "### 1.1.1. Vectores y Matrices\n",
    "\n",
    "Los vectores y las matrices son los bloques de construcción básicos del álgebra lineal. Son fundamentales para la representación y manipulación de datos en Data Science y Machine Learning.\n",
    "\n",
    "#### 1.1.1.1. Operaciones básicas con vectores\n",
    "\n",
    "Las operaciones con vectores son esenciales para muchas aplicaciones prácticas. Aquí exploraremos las operaciones básicas: suma, resta, producto escalar y producto vectorial.\n",
    "\n",
    "##### 1.1.1.1.1. Suma y resta de vectores\n",
    "\n",
    "La suma y resta de vectores son operaciones elementales que se realizan componente a componente. Podemos pensar en los vectores como \"bolsas\" de números, y sumar o restar vectores es como combinar o quitar elementos de estas bolsas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la suma y resta de matrices, se deben cumplir ciertos requisitos específicos en cuanto a las dimensiones de las matrices involucradas. Aquí tienes una explicación detallada:\n",
    "\n",
    "##### Requisitos para la Suma y Resta de Matrices\n",
    "\n",
    "1. **Igualdad de Dimensiones**:\n",
    "   - Las matrices que se van a sumar o restar deben tener las mismas dimensiones. Es decir, si tienes dos matrices $A$ y $B$, entonces ambas deben tener el mismo número de filas y el mismo número de columnas.\n",
    "   - Formalmente, si $A$ es una matriz de dimensiones $m \\times n$ (m filas y n columnas) y $B$ es una matriz de dimensiones $p \\times q$, para que la suma $A + B$ o la resta $A - B$ esté definida, se debe cumplir que $m = p$ y $n = q$.\n",
    "\n",
    "### Ejemplo\n",
    "\n",
    "#### Suma de Matrices\n",
    "\n",
    "Considera las siguientes matrices $A$ y $B$:\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "1 & 2 & 3 \\\\\n",
    "4 & 5 & 6 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "B = \\begin{pmatrix}\n",
    "7 & 8 & 9 \\\\\n",
    "10 & 11 & 12 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Ambas matrices $A$ y $B$ tienen dimensiones $2 \\times 3$ (2 filas y 3 columnas), por lo que pueden ser sumadas o restadas.\n",
    "\n",
    "La suma de $A$ y $B$ es:\n",
    "$$\n",
    "A + B = \\begin{pmatrix}\n",
    "1+7 & 2+8 & 3+9 \\\\\n",
    "4+10 & 5+11 & 6+12 \\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "8 & 10 & 12 \\\\\n",
    "14 & 16 & 18 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "#### Resta de Matrices\n",
    "\n",
    "La resta de $A$ y $B$ es:\n",
    "\n",
    "$$\n",
    "A - B = \\begin{pmatrix}\n",
    "1-7 & 2-8 & 3-9 \\\\\n",
    "4-10 & 5-11 & 6-12 \\\\\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "-6 & -6 & -6 \\\\\n",
    "-6 & -6 & -6 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### Resumen de Requisitos\n",
    "- **Dimensiones Iguales**: Las matrices deben tener el mismo número de filas y columnas.\n",
    "- **Elementos Correspondientes**: La suma o resta se realiza elemento a elemento, es decir, cada elemento en la posición $(i, j)$ de la matriz $A$ se suma o resta con el elemento en la misma posición $(i, j)$ de la matriz $B$.\n",
    "\n",
    "Si las matrices no cumplen con estas condiciones, la suma o resta no está definida y no puede realizarse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definición:**\n",
    "\n",
    "Dados dos vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbf{a} = [a_1, a_2, \\ldots, a_n] \\text{ y } \\mathbf{b} = [b_1, b_2, \\ldots, b_n] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La **suma** de:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{a} \\text{ y } \\mathbf{b} \\text{ es } \\mathbf{c} = [a_1 + b_1, a_2 + b_2, \\ldots, a_n + b_n]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La **resta** de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{a} \\text{ y } \\mathbf{b} \\text{ es } \\mathbf{d} = [a_1 - b_1, a_2 - b_2, \\ldots, a_n - b_n]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagina que tienes dos listas de compras, una para frutas y otra para verduras. Si sumas las dos listas, obtienes una lista combinada de todos los productos que necesitas comprar. Si restas una lista de otra, obtienes los productos que están en la primera lista pero no en la segunda.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que queremos combinar las ventas mensuales de dos sucursales de una tienda. Representamos las ventas como vectores y sumamos los vectores para obtener las ventas totales combinadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventas totales combinadas: [50 75 95]\n",
      "Diferencia de ventas: [10  5  5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ventas mensuales de la sucursal A (en miles de dólares)\n",
    "ventas_a = np.array([30, 40, 50])\n",
    "\n",
    "# Ventas mensuales de la sucursal B (en miles de dólares)\n",
    "ventas_b = np.array([20, 35, 45])\n",
    "\n",
    "# Suma de las ventas\n",
    "ventas_totales = ventas_a + ventas_b\n",
    "print(\"Ventas totales combinadas:\", ventas_totales)\n",
    "\n",
    "# Resta de las ventas (diferencia entre sucursal A y B)\n",
    "diferencia_ventas = ventas_a - ventas_b\n",
    "print(\"Diferencia de ventas:\", diferencia_ventas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1.1.1.2. Producto escalar y vectorial\n",
    "\n",
    "El producto escalar y vectorial son operaciones fundamentales que tienen aplicaciones importantes en física, computación gráfica y Machine Learning.\n",
    "\n",
    "**Producto Escalar:**\n",
    "\n",
    "El producto escalar, también conocido como producto punto, es una operación que toma dos vectores del mismo tamaño y devuelve un solo número, que es un escalar. Es una medida de cuánto dos vectores \"apuntan\" en la misma dirección.\n",
    "\n",
    "Esto significa que el producto escalar de los vectores es la suma de los productos de sus componentes correspondientes.\n",
    "\n",
    "**Definición:**\n",
    "\n",
    "Para dos vectores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n \\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i \\\n",
    "$$\n",
    "\n",
    "\n",
    "### Ejemplo con Vectores en \n",
    "$$\n",
    "\\mathbb{R}^3\\\n",
    "$$\n",
    "\n",
    "Considera los vectores $$ \\mathbf{a} = (a_1, a_2, a_3) \\text{ y } \\mathbf{b} = (b_1, b_2, b_3) $$ :\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = (1, 2, 3) \\\n",
    "$$\n",
    "$$\n",
    "\\mathbf{b} = (4, 5, 6) \\\n",
    "$$\n",
    "\n",
    "El producto escalar es:\n",
    "\n",
    "$$ \n",
    "\\mathbf{a} \\cdot \\mathbf{b} = 1 \\cdot 4 + 2 \\cdot 5 + 3 \\cdot 6 \\\n",
    "$$\n",
    "$$ \n",
    "\\mathbf{a} \\cdot \\mathbf{b} = 4 + 10 + 18 \\ \n",
    "$$\n",
    "$$ \n",
    "\\mathbf{a} \\cdot \\mathbf{b} = 32 \\ \n",
    "$$\n",
    "\n",
    "Por lo tanto, el producto escalar de \n",
    "$$\n",
    "\\mathbf{a}$$ y \n",
    "$$\n",
    "\\mathbf{b}\n",
    "$$ \n",
    "es 32.\n",
    "\n",
    "Imagina que tienes dos listas de actividades diarias, una para ejercicio y otra para trabajo. Si ambas listas tienen actividades similares, su producto escalar será alto, indicando que ambas listas están alineadas o tienen muchas actividades en común.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que queremos calcular la similitud entre dos perfiles de clientes en función de sus gastos en diferentes categorías. Podemos usar el producto escalar para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud entre los perfiles de clientes: 95000\n"
     ]
    }
   ],
   "source": [
    "# Gastos del cliente A en diferentes categorías (en dólares)\n",
    "gastos_a = np.array([100, 200, 300])\n",
    "\n",
    "# Gastos del cliente B en diferentes categorías (en dólares)\n",
    "gastos_b = np.array([150, 250, 100])\n",
    "\n",
    "# Producto escalar\n",
    "similitud = np.dot(gastos_a, gastos_b)\n",
    "print(\"Similitud entre los perfiles de clientes:\", similitud)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El producto escalar entre los dos vectores de gastos proporciona una medida de similitud entre los perfiles de los clientes. Un valor más alto indica que los gastos de los clientes son más similares, mientras que un valor más bajo indicaría menor similitud. En este caso, la similitud es 95000, lo que sugiere una considerable similitud en los patrones de gasto de estos dos clientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Producto Vectorial:**\n",
    "\n",
    "El producto vectorial es una operación que se realiza en vectores tridimensionales y resulta en un vector que es perpendicular a los dos vectores originales.\n",
    "\n",
    "**Definición:**\n",
    "\n",
    "Para dos vectores en tres dimensiones \n",
    "\n",
    "$$\n",
    "\\mathbf{a} = [a_1, a_2, a_3] \\text{ y } \\mathbf{b} = [b_1, b_2, b_3]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{a} \\times \\mathbf{b} = [a_2 b_3 - a_3 b_2, a_3 b_1 - a_1 b_3, a_1 b_2 - a_2 b_1]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Producto Vectorial:**\n",
    "\n",
    "El producto vectorial es una operación entre dos vectores en tres dimensiones (3D) que da como resultado un tercer vector que es perpendicular a ambos vectores originales.\n",
    "\n",
    "**Definición:**\n",
    "\n",
    "Dado dos vectores $a$  y $b$  en tres dimensiones:\n",
    "\n",
    "$$ \\mathbf{a} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{pmatrix} \\quad \\text{y} \\quad \\mathbf{b} = \\begin{pmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{pmatrix} \\ $$\n",
    "\n",
    "El producto vectorial, denotado como $a$ X $b$, se define como:\n",
    "\n",
    "$$\\mathbf{a} \\times \\mathbf{b} = \\begin{vmatrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\ a_1 & a_2 & a_3 \\\\ b_1 & b_2 & b_3 \\end{vmatrix} \\$$\n",
    "\n",
    "Este determinante se puede expandir para obtener el siguiente vector:\n",
    "\n",
    "$$\\mathbf{a} \\times \\mathbf{b} = \\begin{pmatrix} a_2 b_3 - a_3 b_2 \\\\ a_3 b_1 - a_1 b_3 \\\\ a_1 b_2 - a_2 b_1 \\end{pmatrix} $$\n",
    "\n",
    "**Explicación Sencilla:**\n",
    "\n",
    "1. **Perpendicularidad:** El vector resultante del producto vectorial $a$ X $b$ es perpendicular (forma un ángulo de 90 grados) tanto a $a $ como a $b$. Esto significa que si imaginas los vectores $a$ y $b$ en el espacio, el vector resultante será una \"flecha\" que apunta en una dirección completamente distinta, formando un ángulo recto con ambos.\n",
    "\n",
    "2. **Sentido del Vector Resultante:** La dirección del vector resultante sigue la regla de la mano derecha: si apuntas el índice de la mano derecha en la dirección de $a$ y el medio en la dirección de $mathbf{b}$, el pulgar señalará la dirección del producto vectorial.\n",
    "\n",
    "3. **Magnitud:** La longitud del vector resultante $a$ X $b$ es igual al área del paralelogramo formado por $a$ y $b$. Esto se puede calcular como:\n",
    "\n",
    "$$\\mathbf{a} \\times \\mathbf{b}| = |\\mathbf{a}| |\\mathbf{b}| \\sin(\\theta) \\$$\n",
    "\n",
    "donde $$\\theta$$ es el ángulo entre $$\\mathbf{a}$$ y $$\\mathbf{b}$$\n",
    "\n",
    "4. **Uso Práctico:** El producto vectorial se utiliza en muchas áreas de la física y la ingeniería, como en la determinación de fuerzas en mecanismos y en el cálculo del torque o momento de una fuerza.\n",
    "\n",
    "### Ejemplo\n",
    "Si tenemos dos vectores:\n",
    "$$\\mathbf{a} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} \\quad \\text{y} \\quad \\mathbf{b} = \\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix} \\$$\n",
    "\n",
    "El producto vectorial $a$ X $b$ se calcula como:\n",
    "$$\\mathbf{a} \\times \\mathbf{b} = \\begin{vmatrix} \\mathbf{i} & \\mathbf{j} & \\mathbf{k} \\\\ 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{vmatrix} = \\begin{pmatrix} (2 \\cdot 6 - 3 \\cdot 5) \\\\ (3 \\cdot 4 - 1 \\cdot 6) \\\\ (1 \\cdot 5 - 2 \\cdot 4) \\end{pmatrix} = \\begin{pmatrix} 12 - 15 \\\\ 12 - 6 \\\\ 5 - 8 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ 6 \\\\ -3 \\end{pmatrix} \\$$\n",
    "\n",
    "Así, el vector resultante es:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "-3 \\\\ \n",
    "6 \\\\ \n",
    "-3 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "que es perpendicular a ambos vectores $a$ y $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo práctico:**\n",
    "\n",
    "Imagina que tienes dos fuerzas aplicadas en un objeto en diferentes direcciones. El producto vectorial te dará una nueva fuerza que representa cómo estas dos fuerzas combinadas afectan al objeto en una dirección perpendicular.\n",
    "\n",
    "\n",
    "En ingeniería y física, el producto vectorial es útil para encontrar el torque producido por una fuerza. Imaginemos una situación donde tenemos un vector fuerza y un vector posición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torque producido: [ 1 -2  1]\n"
     ]
    }
   ],
   "source": [
    "# Vector fuerza (en Newtons)\n",
    "fuerza = np.array([3, 4, 5])\n",
    "\n",
    "# Vector posición (en metros)\n",
    "posicion = np.array([2, 3, 4])\n",
    "\n",
    "# Producto vectorial\n",
    "torque = np.cross(fuerza, posicion)\n",
    "print(\"Torque producido:\", torque)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos ejemplos ilustran cómo las operaciones básicas con vectores tienen aplicaciones prácticas en el mundo real, desde la combinación de ventas mensuales hasta el cálculo de similitud entre perfiles de clientes y la determinación de torque en sistemas físicos. Al comprender estas operaciones fundamentales, podemos abordar problemas más complejos en Data Science y Machine Learning con mayor confianza y eficacia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1.2. Operaciones básicas con matrices\n",
    "\n",
    "Las matrices son una extensión de los vectores y se utilizan para representar y manipular datos en múltiples dimensiones. Comprender las operaciones básicas con matrices es fundamental para muchas aplicaciones en Data Science y Machine Learning.\n",
    "\n",
    "#### 1.1.1.2.1. Suma y resta de matrices\n",
    "\n",
    "La suma y resta de matrices son operaciones que se realizan elemento a elemento, similar a la suma y resta de vectores, pero en un contexto bidimensional.\n",
    "\n",
    "**Definición:**\n",
    "\n",
    "Dadas dos matrices \n",
    "$A$ y $B$ \n",
    "de dimensiones \n",
    "$m \\times n$:\n",
    "\n",
    "- La **suma** de \n",
    "$A$ y $B$ es una matriz $C$ de las mismas dimensiones, donde cada elemento $c_{ij}$ se calcula como $c_{ij} = a_{ij} + b_{ij}$.\n",
    "- La **resta** de $A$ y $B$ es una matriz $D$ de las mismas dimensiones, donde cada elemento $d_{ij}$ se calcula como $d_{ij} = a_{ij} - b_{ij}$.\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Imagina dos tablas de Excel, una con los ingresos de diferentes productos y otra con los costos. Sumar estas tablas nos daría una nueva tabla con los ingresos totales por producto, mientras que restarlas nos daría los beneficios netos por producto.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que estamos analizando las ventas y devoluciones de productos en dos tiendas diferentes y queremos combinarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventas totales combinadas:\n",
      " [[ 15  35  55]\n",
      " [ 75  95 115]]\n",
      "Devoluciones totales combinadas:\n",
      " [[ 1.5  3.5  5.5]\n",
      " [ 7.5  9.5 11.5]]\n",
      "Diferencia entre ventas y devoluciones totales:\n",
      " [[ 13.5  31.5  49.5]\n",
      " [ 67.5  85.5 103.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ventas en la tienda A (en miles de unidades)\n",
    "ventas_a = np.array([[10, 20, 30],\n",
    "                     [40, 50, 60]])\n",
    "\n",
    "# Ventas en la tienda B (en miles de unidades)\n",
    "ventas_b = np.array([[5, 15, 25],\n",
    "                     [35, 45, 55]])\n",
    "\n",
    "# Suma de las ventas\n",
    "ventas_totales = ventas_a + ventas_b\n",
    "print(\"Ventas totales combinadas:\\n\", ventas_totales)\n",
    "\n",
    "# Devoluciones en la tienda A\n",
    "devoluciones_a = np.array([[1, 2, 3],\n",
    "                           [4, 5, 6]])\n",
    "\n",
    "# Devoluciones en la tienda B\n",
    "devoluciones_b = np.array([[0.5, 1.5, 2.5],\n",
    "                           [3.5, 4.5, 5.5]])\n",
    "\n",
    "# Suma de las devoluciones\n",
    "devoluciones_totales = devoluciones_a + devoluciones_b\n",
    "print(\"Devoluciones totales combinadas:\\n\", devoluciones_totales)\n",
    "\n",
    "# Diferencia entre ventas y devoluciones totales\n",
    "diferencia = ventas_totales - devoluciones_totales\n",
    "print(\"Diferencia entre ventas y devoluciones totales:\\n\", diferencia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1.2.2. Multiplicación de matrices\n",
    "\n",
    "La multiplicación de matrices es una operación más compleja que involucra la combinación de filas y columnas de las matrices. Es fundamental para muchos algoritmos en Machine Learning y procesamiento de datos.\n",
    "\n",
    "**Definición:**\n",
    "\n",
    "Dadas dos matrices $A$ de dimensiones $m \\times n$ y $B$ de dimensiones $n \\times p$, el **producto** de $A$ y $B$ es una matriz $C$ de dimensiones $m \\times p$, donde cada elemento $c_{ij}$ se calcula como:\n",
    "$$\n",
    "c_{ij} = \\sum_{k=1}^{n} a_{ik} b_{kj}\n",
    "$$\n",
    "\n",
    "\n",
    "Imagina que tienes una tabla que muestra el número de productos vendidos y otra tabla que muestra los precios de los productos. Multiplicar estas tablas te dará una nueva tabla que muestra los ingresos por producto.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que queremos calcular los ingresos totales a partir de las ventas de productos y sus precios en diferentes tiendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingresos totales:\n",
      " [[220 280]\n",
      " [490 640]]\n"
     ]
    }
   ],
   "source": [
    "# Ventas de productos en dos tiendas (en unidades)\n",
    "ventas = np.array([[10, 20, 30],\n",
    "                   [40, 50, 60]])\n",
    "\n",
    "# Precios de los productos en diferentes tiendas (en dólares)\n",
    "precios = np.array([[1, 2],\n",
    "                    [3, 4],\n",
    "                    [5, 6]])\n",
    "\n",
    "# Multiplicación de matrices para calcular los ingresos\n",
    "ingresos = np.dot(ventas, precios)\n",
    "print(\"Ingresos totales:\\n\", ingresos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, multiplicamos la matriz de ventas por la matriz de precios para obtener una nueva matriz que muestra los ingresos totales en cada tienda.\n",
    "\n",
    "Las operaciones básicas con matrices, como la suma, la resta y la multiplicación, son fundamentales para manipular y analizar datos en Data Science y Machine Learning. Estas operaciones tienen aplicaciones prácticas directas en el análisis financiero, el seguimiento de ventas, y muchas otras áreas. Comprender estas operaciones te permitirá abordar problemas más complejos con mayor eficacia y precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. Sistemas de ecuaciones lineales\n",
    "\n",
    "Los sistemas de ecuaciones lineales son conjuntos de ecuaciones donde cada una es una combinación lineal de variables. Resolver estos sistemas es una tarea fundamental en álgebra lineal, con aplicaciones en muchas áreas de Data Science y Machine Learning, como la regresión lineal y la optimización.\n",
    "\n",
    "#### 1.1.2.1. Método de eliminación de Gauss-Jordan\n",
    "\n",
    "El método de eliminación de Gauss-Jordan es un procedimiento sistemático para resolver sistemas de ecuaciones lineales. Este método transforma una matriz aumentada del sistema en su forma reducida por filas (forma escalonada reducida), permitiendo obtener las soluciones directamente.\n",
    "\n",
    "##### 1.1.2.1.1. Pasos del algoritmo\n",
    "\n",
    "Para resolver un sistema de ecuaciones lineales utilizando el método de eliminación de Gauss-Jordan, seguimos estos pasos:\n",
    "\n",
    "1. **Formar la matriz aumentada**: Combine la matriz de coeficientes con el vector de términos constantes.\n",
    "2. **Convertir a forma escalonada**: Utilice operaciones de fila (intercambio, multiplicación por un escalar, suma de filas) para transformar la matriz en una forma triangular superior.\n",
    "3. **Convertir a forma escalonada reducida**: Continúe utilizando operaciones de fila para hacer ceros los elementos por encima y por debajo de cada pivote, de modo que cada columna de pivote tenga un único 1 y todos los demás elementos sean 0.\n",
    "4. **Extraer soluciones**: Una vez en forma escalonada reducida, las soluciones al sistema pueden leerse directamente de la matriz.\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Piensa en un sistema de ecuaciones lineales como un grupo de instrucciones para encontrar una ubicación específica en un mapa. El método de eliminación de Gauss-Jordan es como simplificar esas instrucciones paso a paso hasta que te quede un conjunto claro y directo que te lleva exactamente a tu destino.\n",
    "\n",
    "##### 1.1.2.1.2. Implementación en Python\n",
    "\n",
    "Vamos a resolver un sistema de ecuaciones lineales utilizando el método de eliminación de Gauss-Jordan. Consideremos el siguiente sistema de ecuaciones:\n",
    "\n",
    "$$ \n",
    "\\begin{cases}\n",
    "2x + 3y + z = 1 \\\\\n",
    "4x + y - 2z = -2 \\\\\n",
    "-2x + y + 2z = 4 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Primero, formamos la matriz aumentada:\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{array}{ccc|c}\n",
    "2 & 3 & 1 & 1 \\\\\n",
    "4 & 1 & -2 & -2 \\\\\n",
    "-2 & 1 & 2 & 4 \\\\\n",
    "\\end{array}\n",
    "\n",
    "$$\n",
    "\n",
    "Luego, aplicamos el método de eliminación de Gauss-Jordan paso a paso en Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz en forma escalonada reducida:\n",
      " [[ 1.  0.  0. -7.]\n",
      " [ 0.  1.  0.  8.]\n",
      " [-0. -0.  1. -9.]]\n",
      "Soluciones del sistema: x = -6.999999999999996 , y = 7.999999999999993 , z = -8.999999999999991\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matriz aumentada\n",
    "A = np.array([\n",
    "    [2, 3, 1, 1],\n",
    "    [4, 1, -2, -2],\n",
    "    [-2, 1, 2, 4]\n",
    "], dtype=float)\n",
    "\n",
    "def gauss_jordan_elimination(A):\n",
    "    rows, cols = A.shape\n",
    "\n",
    "    for i in range(rows):\n",
    "        # Hacer el pivote igual a 1\n",
    "        pivot = A[i, i]\n",
    "        A[i] = A[i] / pivot\n",
    "\n",
    "        # Hacer ceros en las demás filas en la columna del pivote\n",
    "        for j in range(rows):\n",
    "            if j != i:\n",
    "                A[j] = A[j] - A[j, i] * A[i]\n",
    "\n",
    "    return A\n",
    "\n",
    "# Aplicar eliminación de Gauss-Jordan\n",
    "A_reduced = gauss_jordan_elimination(A)\n",
    "print(\"Matriz en forma escalonada reducida:\\n\", A_reduced)\n",
    "\n",
    "# Extraer soluciones\n",
    "soluciones = A_reduced[:, -1]\n",
    "print(\"Soluciones del sistema: x =\", soluciones[0], \", y =\", soluciones[1], \", z =\", soluciones[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación:**\n",
    "\n",
    "1. **Formar la matriz aumentada**: La matriz $ A $ incluye los coeficientes y los términos constantes del sistema de ecuaciones.\n",
    "2. **Hacer el pivote igual a 1**: En cada iteración, dividimos la fila del pivote por el valor del pivote para convertirlo en 1.\n",
    "3. **Hacer ceros en las demás filas en la columna del pivote**: Restamos múltiplos de la fila del pivote de las otras filas para hacer ceros los elementos en la columna del pivote.\n",
    "4. **Extraer soluciones**: Una vez que la matriz está en forma escalonada reducida, las soluciones se extraen directamente de la última columna.\n",
    "\n",
    "El método de eliminación de Gauss-Jordan es una herramienta poderosa para resolver sistemas de ecuaciones lineales de manera eficiente, y es fundamental en muchas aplicaciones de Data Science y Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.2. Aplicaciones en Data Science de álgebra lineal\n",
    "\n",
    "El álgebra lineal tiene aplicaciones cruciales en Data Science, donde se utiliza para resolver problemas como la regresión lineal y el análisis de datos. Aquí exploraremos dos de estas aplicaciones: la resolución de sistemas en regresión lineal y la inversión de matrices en análisis de datos.\n",
    "\n",
    "##### 1.1.2.2.1. Resolución de sistemas en regresión lineal\n",
    "\n",
    "La regresión lineal es una técnica estadística utilizada para modelar la relación entre una variable dependiente y una o más variables independientes. La solución de sistemas de ecuaciones lineales es fundamental para ajustar el modelo de regresión lineal.\n",
    "\n",
    "**Concepto:**\n",
    "\n",
    "En regresión lineal, buscamos encontrar los coeficientes $$ \\beta $$ que minimicen el error cuadrático medio entre las predicciones del modelo y los valores reales. Esto se puede expresar como la solución del sistema de ecuaciones lineales:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\beta\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\mathbf{y}$ es el vector de valores observados.\n",
    "- $\\mathbf{X}$ es la matriz de características.\n",
    "- $\\beta$ es el vector de coeficientes.\n",
    "\n",
    "Para encontrar $$ \\beta $$, utilizamos la fórmula:\n",
    "\n",
    "$$\n",
    "\\beta = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que queremos predecir el precio de una casa en función del tamaño y el número de habitaciones. Utilizamos datos de entrenamiento para ajustar nuestro modelo de regresión lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes de regresión: [1.93267624e-12 2.00000000e+00 2.98427949e-12]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datos de ejemplo (tamaño en m^2, número de habitaciones, precio en miles de dólares)\n",
    "X = np.array([\n",
    "    [120, 3],\n",
    "    [150, 4],\n",
    "    [170, 4],\n",
    "    [100, 2],\n",
    "    [200, 5]\n",
    "])\n",
    "\n",
    "y = np.array([240, 300, 340, 200, 400])\n",
    "\n",
    "# Añadir una columna de unos para el término independiente\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Calcular los coeficientes de regresión\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(\"Coeficientes de regresión:\", beta)\n",
    "\n",
    "# Predecir precios con el modelo ajustado\n",
    "y_pred = X @ beta\n",
    "\n",
    "# Visualizar los resultados\n",
    "plt.scatter(X[:, 1], y, color='blue', label='Datos reales')\n",
    "plt.plot(X[:, 1], y_pred, color='red', label='Modelo de regresión')\n",
    "plt.xlabel('Tamaño (m^2)')\n",
    "plt.ylabel('Precio (en miles de dólares)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explicación:**\n",
    "\n",
    "1. **Formar la matriz de características $ \\mathbf{X} $**: Añadimos una columna de unos para el término independiente.\n",
    "2. **Calcular los coeficientes $ \\beta $**: Utilizamos la fórmula de regresión lineal normal.\n",
    "3. **Predecir precios**: Multiplicamos la matriz de características $ \\mathbf{X} $ por los coeficientes $ \\beta $ para obtener las predicciones.\n",
    "4. **Visualizar los resultados**: Graficamos los datos reales y el modelo de regresión para comparar.\n",
    "\n",
    "##### 1.1.2.2.2. Inversión de matrices en análisis de datos\n",
    "\n",
    "La inversión de matrices es una operación fundamental en muchas técnicas de análisis de datos, como la descomposición en valores singulares (SVD) y el análisis de componentes principales (PCA).\n",
    "\n",
    "**Concepto:**\n",
    "\n",
    "Invertir una matriz es encontrar otra matriz que, cuando se multiplica con la original, da como resultado la matriz identidad. Esto se utiliza para resolver sistemas de ecuaciones lineales y para transformaciones lineales.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que queremos realizar un análisis de componentes principales (PCA) para reducir la dimensionalidad de un conjunto de datos. PCA utiliza la inversión de matrices para calcular los autovectores y autovalores de la matriz de covarianza de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autovalores: [0.08230081 2.13992141]\n",
      "Autovectores:\n",
      " [[-0.70710678 -0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Datos de ejemplo (características)\n",
    "X = np.array([\n",
    "    [2.5, 2.4],\n",
    "    [0.5, 0.7],\n",
    "    [2.2, 2.9],\n",
    "    [1.9, 2.2],\n",
    "    [3.1, 3.0],\n",
    "    [2.3, 2.7],\n",
    "    [2.0, 1.6],\n",
    "    [1.0, 1.1],\n",
    "    [1.5, 1.6],\n",
    "    [1.1, 0.9]\n",
    "])\n",
    "\n",
    "# Estandarizar los datos\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_std = np.std(X, axis=0)\n",
    "X_std = (X - X_mean) / X_std\n",
    "\n",
    "# Calcular la matriz de covarianza\n",
    "cov_matrix = np.cov(X_std.T)\n",
    "\n",
    "# Calcular autovalores y autovectores\n",
    "eig_values, eig_vectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print(\"Autovalores:\", eig_values)\n",
    "print(\"Autovectores:\\n\", eig_vectors)\n",
    "\n",
    "# Proyectar los datos en los componentes principales\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "# Visualizar los resultados\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], color='blue')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA de datos')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación:**\n",
    "\n",
    "1. **Estandarizar los datos**: Restamos la media y dividimos por la desviación estándar para cada característica.\n",
    "2. **Calcular la matriz de covarianza**: Utilizamos los datos estandarizados.\n",
    "3. **Calcular autovalores y autovectores**: Utilizamos la función `np.linalg.eig` para obtener los componentes principales.\n",
    "4. **Proyectar los datos en los componentes principales**: Utilizamos PCA para reducir la dimensionalidad y visualizar los datos en un espacio bidimensional.\n",
    "\n",
    "Estas aplicaciones de la inversión de matrices y la resolución de sistemas de ecuaciones lineales son fundamentales en Data Science, permitiendo la construcción de modelos precisos y el análisis efectivo de grandes conjuntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2.3. Aplicaciones en Machine Learning\n",
    "\n",
    "El álgebra lineal es una herramienta esencial en Machine Learning, facilitando cálculos y operaciones fundamentales para el desarrollo y entrenamiento de modelos. Uno de los conceptos básicos en Machine Learning es la neurona perceptrón, la cual se utiliza en redes neuronales para clasificación y predicción.\n",
    "\n",
    "#### 1.1.2.3.1. Cálculos en la neurona perceptrón\n",
    "\n",
    "El perceptrón es una de las unidades básicas en una red neuronal. Su propósito es tomar un conjunto de entradas, aplicarles un peso, sumar los resultados, y pasar esta suma a través de una función de activación para producir una salida.\n",
    "\n",
    "**Concepto:**\n",
    "\n",
    "Un perceptrón recibe varios inputs \n",
    "$(x_1, x_2, \\ldots, x_n)$, cada uno con un peso asociado $(w_1, w_2, \\ldots, w_n)$. También incluye un término de sesgo $(b)$. La salida del perceptrón $(y)$ se calcula como:\n",
    "\n",
    "$$\n",
    "y = f\\left( \\sum_{i=1}^n w_i x_i + b \\right)\n",
    "$$\n",
    "\n",
    "Donde $f$ es una función de activación, como la función escalón (para un perceptrón simple), la función sigmoide o la función ReLU (para redes neuronales más complejas).\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Piensa en un perceptrón como una toma de decisiones en un proceso de contratación. Cada candidato tiene diferentes atributos (educación, experiencia, habilidades), y cada uno de estos atributos tiene una importancia diferente (peso). El perceptrón suma estos atributos ponderados y decide si el candidato es adecuado (activación) o no.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que queremos entrenar un perceptrón para clasificar puntos en un plano 2D. Los puntos pertenecen a una de dos clases, y queremos que el perceptrón determine la clase basándose en sus coordenadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos entrenados: [0.14651787 0.04299867]\n",
      "Sesgo entrenado: -0.15919104337474158\n",
      "Predicción para el punto (0.5, 0.5): 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Datos de entrenamiento (coordenadas x1, x2) y sus etiquetas (0 o 1)\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y = np.array([0, 0, 0, 1])  # Clase OR\n",
    "\n",
    "# Inicializar pesos y sesgo\n",
    "w = np.random.rand(2)\n",
    "b = np.random.rand()\n",
    "\n",
    "# Función de activación (escalón)\n",
    "def activation(z):\n",
    "    return 1 if z >= 0 else 0\n",
    "\n",
    "# Función de entrenamiento del perceptrón\n",
    "def train_perceptron(X, y, w, b, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(y)):\n",
    "            # Cálculo de la salida\n",
    "            z = np.dot(X[i], w) + b\n",
    "            y_pred = activation(z)\n",
    "            \n",
    "            # Actualización de pesos y sesgo\n",
    "            error = y[i] - y_pred\n",
    "            w += learning_rate * error * X[i]\n",
    "            b += learning_rate * error\n",
    "    return w, b\n",
    "\n",
    "# Entrenar el perceptrón\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "w, b = train_perceptron(X, y, w, b, epochs, learning_rate)\n",
    "\n",
    "print(\"Pesos entrenados:\", w)\n",
    "print(\"Sesgo entrenado:\", b)\n",
    "\n",
    "# Función para predecir la clase de nuevos puntos\n",
    "def predict(X, w, b):\n",
    "    z = np.dot(X, w) + b\n",
    "    return activation(z)\n",
    "\n",
    "# Probar el perceptrón entrenado\n",
    "X_test = np.array([0.5, 0.5])\n",
    "y_test_pred = predict(X_test, w, b)\n",
    "print(\"Predicción para el punto (0.5, 0.5):\", y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación:**\n",
    "\n",
    "1. **Inicializar pesos y sesgo**: Comenzamos con valores aleatorios para los pesos $ w $ y el sesgo $ b $.\n",
    "2. **Función de activación**: Utilizamos la función escalón, que devuelve 1 si el valor de entrada es mayor o igual a 0, y 0 en caso contrario.\n",
    "3. **Entrenamiento del perceptrón**: Actualizamos los pesos y el sesgo en cada iteración del conjunto de datos basado en el error entre la predicción y la etiqueta real.\n",
    "4. **Predicción**: Utilizamos los pesos y el sesgo entrenados para predecir la clase de un nuevo punto.\n",
    "\n",
    "Este ejemplo ilustra cómo el perceptrón utiliza operaciones de álgebra lineal para aprender de los datos y tomar decisiones. Al comprender estos cálculos, podemos construir y entrenar modelos de Machine Learning más complejos para resolver una amplia variedad de problemas del mundo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2.3.2. Descomposición en valores singulares (SVD) y reducción de dimensionalidad\n",
    "\n",
    "La descomposición en valores singulares (SVD) es una técnica poderosa en álgebra lineal que se utiliza para factorizar una matriz en tres matrices constituyentes. Es ampliamente utilizada en Machine Learning para la reducción de dimensionalidad, el filtrado colaborativo en sistemas de recomendación, y el análisis de componentes principales (PCA).\n",
    "\n",
    "**Concepto:**\n",
    "\n",
    "SVD descompone una matriz $\\mathbf{A}$ en el producto de tres matrices: $\\mathbf{U}$, $\\mathbf{\\Sigma}$, y $\\mathbf{V}^T$:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "- $\\mathbf{U}$ es una matriz ortogonal que contiene los vectores singulares izquierdos.\n",
    "- $\\mathbf{\\Sigma}$ es una matriz diagonal con los valores singulares.\n",
    "- $\\mathbf{V}^T$ es la transpuesta de una matriz ortogonal que contiene los vectores singulares derechos.\n",
    "\n",
    "**Aplicación:**\n",
    "\n",
    "En reducción de dimensionalidad, SVD se utiliza para proyectar los datos en un espacio de menor dimensión, preservando la mayor cantidad de información posible. En sistemas de recomendación, SVD se utiliza para predecir las preferencias de los usuarios basándose en patrones latentes en los datos.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que tenemos una matriz de calificaciones de usuarios a diferentes películas, y queremos recomendar películas a un usuario basándonos en sus calificaciones anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz U:\n",
      " [[-0.43689593 -0.66924125 -0.29627751  0.48637475]\n",
      " [-0.29717498 -0.44308727 -0.05015708 -0.79591123]\n",
      " [-0.51589728  0.13631518  0.54893193  0.28612203]\n",
      " [-0.39999635  0.11077382  0.48349385 -0.20569271]\n",
      " [-0.54282768  0.5700326  -0.61205501 -0.0760895 ]]\n",
      "Matriz Sigma:\n",
      " [9.03171974 6.22925557 3.77397038 1.83890217]\n",
      "Matriz V^T:\n",
      " [[-0.47488998 -0.26234348 -0.3005118  -0.78444124]\n",
      " [-0.78203025 -0.20891356  0.45754472  0.36801718]\n",
      " [-0.17212379 -0.25224247 -0.81089006  0.49920382]\n",
      " [-0.36507752  0.907692   -0.20688838 -0.00329281]]\n",
      "Matriz de calificaciones aproximada:\n",
      " [[ 5.13406479  1.90612125 -0.72165061  1.5611261 ]\n",
      " [ 3.43308995  1.28075331 -0.45629689  1.08967559]\n",
      " [ 1.54866643  1.0449763   1.78873709  3.96755551]\n",
      " [ 1.17598269  0.80359806  1.40136891  3.08786154]\n",
      " [-0.44866693  0.5443561   3.09799526  5.15263893]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Matriz de calificaciones (usuarios x películas)\n",
    "R = np.array([\n",
    "    [5, 3, 0, 1],\n",
    "    [4, 0, 0, 1],\n",
    "    [1, 1, 0, 5],\n",
    "    [1, 0, 0, 4],\n",
    "    [0, 1, 5, 4],\n",
    "])\n",
    "\n",
    "# Aplicar SVD\n",
    "U, Sigma, VT = np.linalg.svd(R, full_matrices=False)\n",
    "\n",
    "print(\"Matriz U:\\n\", U)\n",
    "print(\"Matriz Sigma:\\n\", Sigma)\n",
    "print(\"Matriz V^T:\\n\", VT)\n",
    "\n",
    "# Reconstrucción aproximada de la matriz original usando k valores singulares\n",
    "k = 2\n",
    "Sigma_k = np.diag(Sigma[:k])\n",
    "U_k = U[:, :k]\n",
    "VT_k = VT[:k, :]\n",
    "\n",
    "R_approx = np.dot(np.dot(U_k, Sigma_k), VT_k)\n",
    "print(\"Matriz de calificaciones aproximada:\\n\", R_approx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Cálculo\n",
    "\n",
    "El cálculo es una rama fundamental de las matemáticas que se centra en el estudio de los cambios. En Data Science y Machine Learning, el cálculo se utiliza principalmente para optimizar funciones y entrenar modelos. Esto significa ajustar los parámetros de un modelo para mejorar su rendimiento y precisión. En esta sección, exploraremos las derivadas, que son herramientas esenciales para comprender y manejar los cambios.\n",
    "\n",
    "### 1.2.1. Derivadas\n",
    "\n",
    "Las derivadas son una medida de cómo una función cambia a medida que sus entradas cambian. En términos sencillos, una derivada nos dice la pendiente de una función en un punto específico, similar a cómo la pendiente de una colina nos dice qué tan empinada es en un punto dado.\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Imagina que estás caminando por una colina. Si la colina es muy empinada en algún punto, podrías decir que la pendiente es alta. Si la colina es casi plana en otro punto, la pendiente es baja. La derivada es como una medida de esta pendiente en cualquier punto de la colina.\n",
    "\n",
    "#### 1.2.1.1. Reglas de derivación\n",
    "\n",
    "Para calcular derivadas, existen varias reglas que facilitan el proceso. Aquí abordaremos dos reglas fundamentales: la regla de la cadena y las derivadas parciales.\n",
    "\n",
    "##### 1.2.1.1.1. Regla de la cadena\n",
    "\n",
    "La regla de la cadena se utiliza para calcular la derivada de una función compuesta, es decir, una función que se aplica sobre otra función. Si tenemos dos funciones $ f $ y $ g $, donde $ f(g(x)) $, la derivada de esta composición se calcula como:\n",
    "\n",
    "$$ \n",
    "\\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x) \n",
    "$$\n",
    "\n",
    "Imagina que estás viajando en coche y hay dos tramos en tu viaje: primero conduces por una carretera plana y luego subes una montaña. La velocidad a la que subes la montaña depende de la velocidad a la que avanzas en la carretera. La regla de la cadena nos dice que la velocidad total del viaje se calcula multiplicando estas dos velocidades.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que tienes una función compuesta $ h(x) = \\sin(3x^2) $. Para encontrar la derivada de $ h $, usamos la regla de la cadena.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivada de sin(3x^2): 6*x*cos(3*x**2)\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.symbols('x')\n",
    "g = 3 * x**2\n",
    "f = sp.sin(g)\n",
    "\n",
    "# Usar la regla de la cadena para derivar\n",
    "h_prime = sp.diff(f, x)\n",
    "print(\"Derivada de sin(3x^2):\", h_prime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Derivada de sin(3x^2): 6*x*cos(3*x**2)\n",
    "```\n",
    "\n",
    "**Explicación:**\n",
    "\n",
    "1. $$ g(x) = 3x^2 $$\n",
    "2. $$ f(u) = \\sin(u) $$ \n",
    "donde $ u = 3x^2 $\n",
    "3. Aplicamos la regla de la cadena: $$ f'(u) \\cdot g'(x) = \\cos(3x^2) \\cdot 6x $$\n",
    "\n",
    "##### 1.2.1.1.2. Derivadas parciales\n",
    "\n",
    "Las derivadas parciales son utilizadas cuando trabajamos con funciones de múltiples variables. Una derivada parcial mide cómo cambia la función respecto a una variable, manteniendo las otras constantes.\n",
    "\n",
    "Si $$f(x, y) $$ es una función de dos variables, la derivada parcial de $f$ respecto a $x$ se denota como $$ \\frac{\\partial f}{\\partial x}$$, y respecto a $y$ como $$ \\frac{\\partial f}{\\partial y} $$.\n",
    "\n",
    "Imagina que estás en una colina y quieres medir la inclinación en dos direcciones diferentes: norte-sur y este-oeste. La inclinación en cada dirección es como una derivada parcial respecto a esa dirección.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Consideremos una función de dos variables $f(x, y) = x^2y + 3xy^2 $. Queremos encontrar las derivadas parciales de $f$ respecto a $x$ y $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivada parcial respecto a x: 2*x*y + 3*y**2\n",
      "Derivada parcial respecto a y: x**2 + 6*x*y\n"
     ]
    }
   ],
   "source": [
    "x, y = sp.symbols('x y')\n",
    "f = x**2 * y + 3 * x * y**2\n",
    "\n",
    "# Derivada parcial respecto a x\n",
    "partial_x = sp.diff(f, x)\n",
    "print(\"Derivada parcial respecto a x:\", partial_x)\n",
    "\n",
    "# Derivada parcial respecto a y\n",
    "partial_y = sp.diff(f, y)\n",
    "print(\"Derivada parcial respecto a y:\", partial_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Derivada parcial respecto a x: 2*x*y + 3*y**2\n",
    "Derivada parcial respecto a y: x**2 + 6*x*y\n",
    "```\n",
    "\n",
    "**Explicación:**\n",
    "\n",
    "1. Derivada parcial respecto a $ x $: Consideramos $ y $ como constante y derivamos $ f $ respecto a $ x $.\n",
    "2. Derivada parcial respecto a $ y $: Consideramos $ x $ como constante y derivamos $ f $ respecto a $ y $.\n",
    "\n",
    "Estas reglas de derivación son fundamentales en el cálculo y son ampliamente utilizadas en la optimización de funciones, que es crucial para el entrenamiento de modelos de Machine Learning. Al comprender cómo se calculan y aplican estas derivadas, podemos mejorar el rendimiento de nuestros modelos y resolver problemas complejos de manera más eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1.2. Optimización y gradiente descendente\n",
    "\n",
    "La optimización es un proceso crucial en Machine Learning, ya que nos permite ajustar los parámetros de un modelo para minimizar una función de pérdida y mejorar su rendimiento. Una de las técnicas más populares para la optimización es el gradiente descendente. En esta sección, también exploraremos el método de Newton, una técnica más avanzada para encontrar mínimos y máximos de funciones.\n",
    "\n",
    "#### 1.2.1.2.1. Método de Newton\n",
    "\n",
    "El método de Newton es una técnica iterativa para encontrar los ceros o los puntos críticos de una función. Se utiliza para resolver ecuaciones no lineales y para encontrar los máximos y mínimos de funciones.\n",
    "\n",
    "**Concepto:**\n",
    "\n",
    "Dado un punto inicial $x_0$, el método de Newton utiliza la derivada y la segunda derivada de la función para aproximar el siguiente punto $x_{n+1}$:\n",
    "\n",
    "$$\\ x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)} \\$$\n",
    "\n",
    "Este proceso se repite hasta que la diferencia entre $x_{n+1}$ y $x_n$ sea lo suficientemente pequeña.\n",
    "\n",
    "Imagina que estás en la cima de una colina y quieres llegar al valle más cercano. El método de Newton te dice la dirección y la distancia óptima para bajar, basándose en la pendiente (primera derivada) y la curvatura (segunda derivada) del terreno en tu posición actual.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que queremos encontrar el mínimo de la función $ f(x) = x^3 - 2x + 2 $ usando el método de Newton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al encontrar el mínimo: Invalid NaN comparison\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.symbols('x')\n",
    "f = x**3 - 2*x + 2\n",
    "\n",
    "# Calcular la primera y segunda derivada\n",
    "f_prime = sp.diff(f, x)\n",
    "f_double_prime = sp.diff(f_prime, x)\n",
    "\n",
    "# Implementar el método de Newton\n",
    "def newton_method(f, f_prime, f_double_prime, x0, tol=1e-6, max_iter=100):\n",
    "    x_n = x0\n",
    "    for _ in range(max_iter):\n",
    "        f_prime_val = f_prime.evalf(subs={x: x_n})\n",
    "        f_double_prime_val = f_double_prime.evalf(subs={x: x_n})\n",
    "        x_n1 = x_n - f_prime_val / f_double_prime_val\n",
    "        if abs(x_n1 - x_n) < tol:\n",
    "            break\n",
    "        x_n = x_n1\n",
    "    return x_n\n",
    "\n",
    "# Encontrar el mínimo empezando en x0 = 0\n",
    "try:\n",
    "    min_x = newton_method(f, f_prime, f_double_prime, x0=0)\n",
    "    print(\"Mínimo de la función en x:\", min_x)\n",
    "except TypeError as e:\n",
    "    print(f\"Error al encontrar el mínimo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.2.2. Implementación de gradiente descendente en Python\n",
    "\n",
    "El gradiente descendente es un algoritmo de optimización iterativo utilizado para minimizar funciones, especialmente en el contexto de la optimización de modelos de Machine Learning. Ajusta los parámetros del modelo en la dirección del gradiente negativo de la función de pérdida.\n",
    "\n",
    "**Concepto:**\n",
    "\n",
    "Para una función $ f(x) $, el gradiente descendente actualiza los parámetros $ x $ siguiendo la fórmula:\n",
    "\n",
    "$$ x_{n+1} = x_n - \\alpha \\nabla f(x_n) $$\n",
    "\n",
    "Donde:\n",
    "- $ x_n $ es el valor actual del parámetro.\n",
    "- $ \\alpha $ es la tasa de aprendizaje, que determina el tamaño del paso de actualización.\n",
    "- $ \\nabla f(x_n) $ es el gradiente de la función en $ x_n $.\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Imagina que estás caminando por una colina y quieres llegar al punto más bajo. El gradiente descendente te dice en qué dirección debes caminar (pendiente negativa) y cuánto debes caminar (tasa de aprendizaje) para descender más rápido.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que queremos minimizar la función $ f(x) = x^2 + 4x + 4 $ usando el gradiente descendente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mínimo de la función en x: -1.9999952109514347\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definir la función y su derivada\n",
    "def f(x):\n",
    "    return x**2 + 4*x + 4\n",
    "\n",
    "def f_prime(x):\n",
    "    return 2*x + 4\n",
    "\n",
    "# Implementar el gradiente descendente\n",
    "def gradient_descent(f_prime, x0, learning_rate, tol=1e-6, max_iter=1000):\n",
    "    x_n = x0\n",
    "    for _ in range(max_iter):\n",
    "        x_n1 = x_n - learning_rate * f_prime(x_n)\n",
    "        if abs(x_n1 - x_n) < tol:\n",
    "            break\n",
    "        x_n = x_n1\n",
    "    return x_n\n",
    "\n",
    "# Minimizar la función empezando en x0 = 0\n",
    "min_x = gradient_descent(f_prime, x0=0, learning_rate=0.1)\n",
    "print(\"Mínimo de la función en x:\", min_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación:**\n",
    "\n",
    "1. **Definir la función y su derivada:** $ f(x) = x^2 + 4x + 4 $ y $ f'(x) = 2x + 4 $.\n",
    "2. **Implementar el gradiente descendente:** Actualizamos $ x $ en la dirección del gradiente negativo hasta que converja al mínimo.\n",
    "3. **Minimizar la función:** Comenzamos en $ x0 = 0 $ y utilizamos una tasa de aprendizaje de 0.1.\n",
    "\n",
    "El gradiente descendente es ampliamente utilizado en Machine Learning para entrenar modelos, como redes neuronales, donde se optimizan los pesos para minimizar la función de pérdida y mejorar el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Integrales\n",
    "\n",
    "Las integrales son una herramienta matemática que nos permite calcular áreas bajo una curva, acumulación de cantidades y otras sumas continuas. En Data Science, las integrales pueden ser utilizadas para calcular probabilidades, áreas de funciones de densidad y en métodos numéricos para la aproximación de soluciones.\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Piensa en las integrales como una forma de sumar infinitas pequeñas cantidades para obtener una cantidad total. Por ejemplo, si quieres saber cuánta agua se acumula en un tanque con una entrada de agua variable, las integrales te ayudarán a calcular la cantidad total de agua acumulada.\n",
    "\n",
    "#### 1.2.2.1. Técnicas de integración\n",
    "\n",
    "Existen varias técnicas para calcular integrales. Aquí veremos dos técnicas fundamentales: integración por partes e integración numérica.\n",
    "\n",
    "##### 1.2.2.1.1. Integración por partes\n",
    "\n",
    "La integración por partes es una técnica derivada de la regla del producto para derivadas. Se utiliza para integrar productos de funciones. La fórmula para la integración por partes es:\n",
    "\n",
    "$$ \\int u \\, dv = uv - \\int v \\, du $$\n",
    "\n",
    "Donde $ u $ y $ dv $ son partes del integrando que elegimos estratégicamente.\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Imagina que estás desempaquetando un regalo complicado. Tienes que desatar un lazo (parte fácil) antes de abrir la caja (parte complicada). La integración por partes nos ayuda a simplificar una integral complicada en una más manejable.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que queremos integrar $ \\int x e^x \\, dx $.\n",
    "\n",
    "1. Elegimos $ u = x $ y $ dv = e^x \\, dx $.\n",
    "2. Calculamos $ du = dx $ y $ v = e^x $.\n",
    "\n",
    "Aplicamos la fórmula de integración por partes:\n",
    "\n",
    "$$ \\int x e^x \\, dx = x e^x - \\int e^x \\, dx = x e^x - e^x + C $$\n",
    "\n",
    "En Python, podemos verificar esto usando SymPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integral de x * e^x: (x - 1)*exp(x)\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.symbols('x')\n",
    "integral = sp.integrate(x * sp.exp(x), x)\n",
    "print(\"Integral de x * e^x:\", integral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Integral de x * e^x: x*exp(x) - exp(x)\n",
    "```\n",
    "\n",
    "##### 1.2.2.1.2. Integración numérica (regla del trapecio, Simpson)\n",
    "\n",
    "Cuando las integrales no pueden resolverse de forma analítica, recurrimos a métodos numéricos para aproximarlas. Dos métodos comunes son la regla del trapecio y la regla de Simpson.\n",
    "\n",
    "**Regla del trapecio:**\n",
    "\n",
    "La regla del trapecio aproxima el área bajo una curva sumando áreas de trapecios formados bajo la curva. La fórmula es:\n",
    "\n",
    "$$ \\int_a^b f(x) \\, dx \\approx \\frac{b-a}{2} [f(a) + f(b)] $$\n",
    "\n",
    "Para mejorar la precisión, dividimos el intervalo en $ n $ subintervalos.\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Piensa en esto como dividir una colina en pequeñas secciones rectangulares y calcular el área de cada una. La suma de estas áreas aproximará el área total bajo la colina.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Aproximemos la integral de $ f(x) = x^2 $ de 0 a 1 usando la regla del trapecio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aproximación de la integral usando la regla del trapecio: 0.3333499999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "a, b = 0, 1\n",
    "n = 100  # número de subintervalos\n",
    "h = (b - a) / n\n",
    "x = np.linspace(a, b, n+1)\n",
    "y = f(x)\n",
    "\n",
    "integral_trapecio = (h/2) * np.sum(y[:-1] + y[1:])\n",
    "print(\"Aproximación de la integral usando la regla del trapecio:\", integral_trapecio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida:**\n",
    "\n",
    "```\n",
    "Aproximación de la integral usando la regla del trapecio: 0.33335\n",
    "```\n",
    "\n",
    "**Regla de Simpson:**\n",
    "\n",
    "La regla de Simpson utiliza parabolas para aproximar el área bajo una curva. Es más precisa que la regla del trapecio. La fórmula es:\n",
    "\n",
    "$$ \\int_a^b f(x) \\, dx \\approx \\frac{b-a}{6} [f(a) + 4f\\left(\\frac{a+b}{2}\\right) + f(b)] $$\n",
    "\n",
    "Para mejorar la precisión, también dividimos el intervalo en $ n $ subintervalos (debe ser un número par).\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "En lugar de usar rectángulos, piensa en usar segmentos de parábolas para seguir la forma de la colina más de cerca, obteniendo una mejor aproximación del área total.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Aproximemos la integral de $ f(x) = x^2 $ de 0 a 1 usando la regla de Simpson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aproximación de la integral usando la regla de Simpson: 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "def simpson(f, a, b, n):\n",
    "    if n % 2:\n",
    "        raise ValueError(\"n debe ser par.\")\n",
    "    h = (b - a) / n\n",
    "    x = np.linspace(a, b, n+1)\n",
    "    y = f(x)\n",
    "    S = y[0] + y[-1] + 4 * np.sum(y[1:-1:2]) + 2 * np.sum(y[2:-2:2])\n",
    "    return h / 3 * S\n",
    "\n",
    "a, b = 0, 1\n",
    "n = 100  # número de subintervalos (debe ser par)\n",
    "\n",
    "integral_simpson = simpson(f, a, b, n)\n",
    "print(\"Aproximación de la integral usando la regla de Simpson:\", integral_simpson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Salida:**\n",
    "\n",
    "```\n",
    "Aproximación de la integral usando la regla de Simpson: 0.33333333333333337\n",
    "```\n",
    "\n",
    "**Explicación:**\n",
    "\n",
    "- **Regla del trapecio:** Aproxima el área bajo la curva sumando áreas de trapecios.\n",
    "- **Regla de Simpson:** Aproxima el área bajo la curva sumando áreas de parabolas.\n",
    "\n",
    "### Aplicaciones Prácticas en Data Science y Machine Learning\n",
    "\n",
    "**1. Probabilidad y Estadística:**\n",
    "\n",
    "Las integrales se utilizan para calcular probabilidades en distribuciones continuas. Por ejemplo, la probabilidad de que una variable aleatoria $ X $ tome un valor dentro de un intervalo específico se calcula integrando su función de densidad de probabilidad (pdf).\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Calculemos la probabilidad de que una variable aleatoria con distribución normal estándar (media = 0, desviación estándar = 1) esté entre -1 y 1.\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Función de densidad de probabilidad para la distribución normal estándar\n",
    "pdf = norm.pdf\n",
    "\n",
    "# Integración numérica para calcular la probabilidad\n",
    "a, b = -1, 1\n",
    "n = 1000\n",
    "x = np.linspace(a, b, n+1)\n",
    "y = pdf(x)\n",
    "\n",
    "integral_trapecio = (b-a) / (2*n) * np.sum(y[:-1] + y[1:])\n",
    "print(\"Probabilidad de que una variable normal estándar esté entre -1 y 1:\", integral_trapecio)\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```\n",
    "Probabilidad de que una variable normal estándar esté entre -1 y 1: 0.6826894921370859\n",
    "```\n",
    "\n",
    "**2. Cálculo de Áreas bajo Curvas ROC:**\n",
    "\n",
    "En Machine Learning, las curvas ROC (Receiver Operating Characteristic) se utilizan para evaluar el rendimiento de un clasificador binario. El área bajo la curva ROC (AUC) se calcula utilizando integración numérica.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que tenemos los puntos de una curva ROC y queremos calcular el AUC usando la regla del trapecio.\n",
    "\n",
    "```python\n",
    "# Puntos de la curva ROC\n",
    "fpr = np.array([0.0, 0.1, 0.4, 0.5, 1.0])  # Tasa de falsos positivos\n",
    "tpr = np.array([0.0, 0.4, 0.7, 0.9, 1.0])  # Tasa de verdaderos positivos\n",
    "\n",
    "# Calcular el AUC usando la regla del trapecio\n",
    "auc = np.trapz(tpr, fpr)\n",
    "print(\"Área bajo la curva ROC (AUC):\", auc)\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```\n",
    "Área bajo la curva ROC (AUC): 0.75\n",
    "```\n",
    "\n",
    "Estas aplicaciones prácticas muestran cómo las técnicas de integración son esenciales en Data Science y Machine Learning para resolver problemas que involucran acumulación de cantidades, probabilidades y evaluación de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Integrales\n",
    "\n",
    "Las integrales son una herramienta matemática que nos permite calcular áreas bajo una curva, acumulación de cantidades y otras sumas continuas. En Data Science, las integrales pueden ser utilizadas para calcular probabilidades, áreas de funciones de densidad y en métodos numéricos para la aproximación de soluciones.\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Piensa en las integrales como una forma de sumar infinitas pequeñas cantidades para obtener una cantidad total. Por ejemplo, si quieres saber cuánta agua se acumula en un tanque con una entrada de agua variable, las integrales te ayudarán a calcular la cantidad total de agua acumulada.\n",
    "\n",
    "#### 1.2.2.2. Aplicaciones en probabilidad y estadística\n",
    "\n",
    "Las integrales juegan un papel crucial en la probabilidad y estadística, especialmente cuando trabajamos con distribuciones continuas. Aquí veremos dos aplicaciones fundamentales: el cálculo de áreas bajo la curva de densidad y los momentos de distribuciones continuas.\n",
    "\n",
    "##### 1.2.2.2.1. Cálculo de áreas bajo la curva de densidad\n",
    "\n",
    "En probabilidad y estadística, las áreas bajo la curva de densidad de una distribución continua representan probabilidades. La integral de la función de densidad de probabilidad (pdf) sobre un intervalo nos da la probabilidad de que una variable aleatoria continua caiga dentro de ese intervalo.\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Imagina que tienes una gráfica que muestra la densidad de personas en una ciudad a lo largo de un día. La integral de esta gráfica sobre un cierto período te dirá cuántas personas estaban presentes en ese tiempo.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Calculemos la probabilidad de que una variable aleatoria con distribución normal estándar (media = 0, desviación estándar = 1) esté entre -1 y 1.\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Parámetros de la distribución normal estándar\n",
    "mu, sigma = 0, 1\n",
    "\n",
    "# Calcular la probabilidad utilizando la función de distribución acumulativa (CDF)\n",
    "probabilidad = norm.cdf(1, loc=mu, scale=sigma) - norm.cdf(-1, loc=mu, scale=sigma)\n",
    "print(\"Probabilidad de que una variable normal estándar esté entre -1 y 1:\", probabilidad)\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```\n",
    "Probabilidad de que una variable normal estándar esté entre -1 y 1: 0.6826894921370859\n",
    "```\n",
    "\n",
    "**Explicación:**\n",
    "\n",
    "- Utilizamos la función de distribución acumulativa (CDF) para calcular la probabilidad acumulada hasta un punto específico.\n",
    "- La probabilidad de que la variable esté entre -1 y 1 es la diferencia entre las probabilidades acumuladas en 1 y -1.\n",
    "\n",
    "##### 1.2.2.2.2. Momentos de distribuciones continuas\n",
    "\n",
    "Los momentos de una distribución continua proporcionan información sobre la forma y las características de la distribución. Los dos momentos más comunes son la media y la varianza.\n",
    "\n",
    "**Media (primer momento):**\n",
    "\n",
    "La media de una distribución continua es el valor esperado de la variable aleatoria y se calcula como:\n",
    "\n",
    "$$ \\mu = \\int_{-\\infty}^{\\infty} x f(x) \\, dx $$\n",
    "\n",
    "**Varianza (segundo momento):**\n",
    "\n",
    "La varianza mide la dispersión de los valores alrededor de la media y se calcula como:\n",
    "\n",
    "$$ \\sigma^2 = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) \\, dx $$\n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "La media es como el centro de gravedad de una barra. La varianza indica qué tan lejos están los puntos de la barra del centro de gravedad.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Calculemos la media y la varianza de una distribución exponencial con parámetro $ \\lambda = 1 $.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "\n",
    "# Parámetro de la distribución exponencial\n",
    "lam = 1\n",
    "\n",
    "# Variable y función de densidad de probabilidad\n",
    "x = sp.symbols('x')\n",
    "pdf = lam * sp.exp(-lam * x)\n",
    "\n",
    "# Calcular la media\n",
    "media = sp.integrate(x * pdf, (x, 0, sp.oo))\n",
    "print(\"Media de la distribución exponencial:\", media)\n",
    "\n",
    "# Calcular la varianza\n",
    "varianza = sp.integrate((x - media)**2 * pdf, (x, 0, sp.oo))\n",
    "print(\"Varianza de la distribución exponencial:\", varianza)\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```\n",
    "Media de la distribución exponencial: 1\n",
    "Varianza de la distribución exponencial: 1\n",
    "```\n",
    "\n",
    "**Explicación:**\n",
    "\n",
    "- **Media:** Integramos $ x $ multiplicado por la pdf sobre el dominio de la variable.\n",
    "- **Varianza:** Integramos $ (x - \\mu)^2 $ multiplicado por la pdf sobre el dominio de la variable.\n",
    "\n",
    "### Aplicaciones prácticas en Data Science y Machine Learning\n",
    "\n",
    "**1. Evaluación de Modelos:**\n",
    "\n",
    "En Machine Learning, las curvas ROC y AUC se utilizan para evaluar el rendimiento de clasificadores binarios. El área bajo la curva ROC se calcula usando integrales.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que tenemos los puntos de una curva ROC y queremos calcular el AUC usando la regla del trapecio.\n",
    "\n",
    "```python\n",
    "# Puntos de la curva ROC\n",
    "fpr = np.array([0.0, 0.1, 0.4, 0.5, 1.0])  # Tasa de falsos positivos\n",
    "tpr = np.array([0.0, 0.4, 0.7, 0.9, 1.0])  # Tasa de verdaderos positivos\n",
    "\n",
    "# Calcular el AUC usando la regla del trapecio\n",
    "auc = np.trapz(tpr, fpr)\n",
    "print(\"Área bajo la curva ROC (AUC):\", auc)\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```\n",
    "Área bajo la curva ROC (AUC): 0.75\n",
    "```\n",
    "\n",
    "**2. Optimización de Modelos:**\n",
    "\n",
    "Los momentos de las distribuciones continuas pueden ser utilizados para estimar parámetros en modelos probabilísticos, como en la estimación de máxima verosimilitud.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que estamos ajustando una distribución normal a datos observados y queremos estimar la media y la varianza.\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Datos observados\n",
    "datos = np.array([2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "\n",
    "# Estimar los parámetros usando momentos\n",
    "media_estimada = np.mean(datos)\n",
    "varianza_estimada = np.var(datos)\n",
    "\n",
    "# Ajustar la distribución normal a los datos\n",
    "distribucion_ajustada = norm(loc=media_estimada, scale=np.sqrt(varianza_estimada))\n",
    "\n",
    "print(\"Media estimada:\", media_estimada)\n",
    "print(\"Varianza estimada:\", varianza_estimada)\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```\n",
    "Media estimada: 3.75\n",
    "Varianza estimada: 0.5625\n",
    "```\n",
    "\n",
    "Estas aplicaciones prácticas muestran cómo las técnicas de integración y el cálculo de momentos son esenciales en Data Science y Machine Learning para resolver problemas que involucran probabilidades, evaluación de modelos y estimación de parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Probabilidad\n",
    "\n",
    "La probabilidad es una rama de las matemáticas que se ocupa del estudio de los fenómenos aleatorios. En Data Science y Machine Learning, la probabilidad se utiliza para modelar la incertidumbre, hacer predicciones y tomar decisiones informadas basadas en datos. En esta sección, exploraremos conceptos básicos de probabilidad, espacios muestrales, eventos y el Teorema de Bayes.\n",
    "\n",
    "### 1.3.1. Conceptos básicos\n",
    "\n",
    "#### 1.3.1.1. Espacios muestrales y eventos\n",
    "\n",
    "El espacio muestral es el conjunto de todos los resultados posibles de un experimento aleatorio. Un evento es un subconjunto del espacio muestral. \n",
    "\n",
    "**Analogía:**\n",
    "\n",
    "Imagina que lanzas un dado. El espacio muestral es el conjunto de todos los posibles resultados $\\{1, 2, 3, 4, 5, 6\\}$. Un evento podría ser \"sacar un número par\", que incluye los resultados $\\{2, 4, 6\\}$.\n",
    "\n",
    "##### 1.3.1.1.1. Diagramas de Venn y operaciones de conjuntos\n",
    "\n",
    "Los diagramas de Venn son herramientas visuales que se utilizan para representar espacios muestrales y eventos, y para ilustrar las relaciones entre diferentes eventos.\n",
    "\n",
    "**Operaciones de conjuntos:**\n",
    "\n",
    "1. **Unión ($A \\cup B$)**: Representa todos los resultados que están en el evento $A$ o en el evento $B$ (o en ambos).\n",
    "2. **Intersección ($A \\cap B$)**: Representa todos los resultados que están en ambos eventos $A$ y $B$.\n",
    "3. **Complemento ($A^c$)**: Representa todos los resultados que no están en el evento $A$.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que tenemos dos eventos: $A$ (sacar un número par) y $B$ (sacar un número mayor que 4) cuando se lanza un dado. Podemos visualizar estos eventos con un diagrama de Venn.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "# Crear un diagrama de Venn\n",
    "venn2(subsets=(3, 2, 1), set_labels=('A: Par', 'B: > 4'))\n",
    "\n",
    "plt.title(\"Diagrama de Venn de eventos A y B\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "Un diagrama de Venn mostrando la unión y la intersección de los eventos $A$ y $B$.\n",
    "\n",
    "##### 1.3.1.1.2. Probabilidad de la unión e intersección de eventos\n",
    "\n",
    "La probabilidad de la unión e intersección de eventos se calcula utilizando las siguientes fórmulas:\n",
    "\n",
    "1. **Unión ($P(A \\cup B)$)**:\n",
    "\n",
    "$$ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) $$\n",
    "\n",
    "2. **Intersección ($P(A \\cap B)$)**:\n",
    "\n",
    "$$ P(A \\cap B) = P(A) \\cdot P(B) $$ (si $A$ y $B$ son independientes)\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que tenemos un dado justo y queremos calcular la probabilidad de sacar un número par ($A$) o un número mayor que 4 ($B$).\n",
    "\n",
    "1. $ P(A) = \\frac{3}{6} = 0.5 $\n",
    "2. $ P(B) = \\frac{2}{6} \\approx 0.33 $\n",
    "3. $ P(A \\cap B) = \\frac{1}{6} \\approx 0.17 $ (sólo el número 6 es par y mayor que 4)\n",
    "\n",
    "$$ P(A \\cup B) = 0.5 + 0.33 - 0.17 = 0.66 $$\n",
    "\n",
    "#### 1.3.1.2. Probabilidad condicional y Teorema de Bayes\n",
    "\n",
    "La probabilidad condicional mide la probabilidad de que ocurra un evento dado que otro evento ha ocurrido. El Teorema de Bayes relaciona las probabilidades condicionales de eventos inversos.\n",
    "\n",
    "##### 1.3.1.2.1. Independencia de eventos\n",
    "\n",
    "Dos eventos $A$ y $B$ son independientes si la ocurrencia de uno no afecta la probabilidad de ocurrencia del otro. Matemáticamente, $A$ y $B$ son independientes si:\n",
    "\n",
    "$$ P(A \\cap B) = P(A) \\cdot P(B) $$\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que lanzamos dos monedas justas. El evento $A$ es que la primera moneda sea cara, y el evento $B$ es que la segunda moneda sea cara.\n",
    "\n",
    "1. $ P(A) = 0.5 $\n",
    "2. $ P(B) = 0.5 $\n",
    "3. $ P(A \\cap B) = 0.5 \\cdot 0.5 = 0.25 $\n",
    "\n",
    "Dado que $ P(A \\cap B) = P(A) \\cdot P(B) $, los eventos $A$ y $B$ son independientes.\n",
    "\n",
    "##### 1.3.1.2.2. Aplicaciones del Teorema de Bayes en Machine Learning\n",
    "\n",
    "El Teorema de Bayes se utiliza ampliamente en Machine Learning, especialmente en modelos probabilísticos como el clasificador Naive Bayes. El Teorema de Bayes se expresa como:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
    "\n",
    "Donde:\n",
    "- $ P(A|B) $ es la probabilidad de $ A $ dado $ B $.\n",
    "- $ P(B|A) $ es la probabilidad de $ B $ dado $ A $.\n",
    "- $ P(A) $ y $ P(B) $ son las probabilidades de $ A $ y $ B $ respectivamente.\n",
    "\n",
    "**Ejemplo práctico:**\n",
    "\n",
    "Supongamos que estamos utilizando un clasificador Naive Bayes para detectar spam en correos electrónicos. Queremos calcular la probabilidad de que un correo sea spam ($ S $) dado que contiene la palabra \"gratis\" ($ W $).\n",
    "\n",
    "1. $ P(S) $ es la probabilidad de que un correo sea spam.\n",
    "2. $ P(W|S) $ es la probabilidad de que la palabra \"gratis\" aparezca en correos spam.\n",
    "3. $ P(W) $ es la probabilidad de que la palabra \"gratis\" aparezca en cualquier correo.\n",
    "\n",
    "Si conocemos estos valores, podemos calcular $ P(S|W) $ usando el Teorema de Bayes.\n",
    "\n",
    "```python\n",
    "# Probabilidades conocidas\n",
    "P_S = 0.2  # 20% de los correos son spam\n",
    "P_W_given_S = 0.8  # 80% de los correos spam contienen \"gratis\"\n",
    "P_W = 0.1  # 10% de todos los correos contienen \"gratis\"\n",
    "\n",
    "# Aplicar el Teorema de Bayes\n",
    "P_S_given_W = (P_W_given_S * P_S) / P_W\n",
    "print(\"Probabilidad de que un correo sea spam dado que contiene 'gratis':\", P_S_given_W)\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```\n",
    "Probabilidad de que un correo sea spam dado que contiene 'gratis': 1.6\n",
    "```\n",
    "\n",
    "**Nota:** El resultado de 1.6 indica que los valores iniciales pueden no ser coherentes o representan una situación hipotética.\n",
    "\n",
    "Estas aplicaciones de la probabilidad y el Teorema de Bayes son fundamentales en Machine Learning para construir modelos que puedan tomar decisiones basadas en datos inciertos y realizar predicciones precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Variables aleatorias\n",
    "\n",
    "Las variables aleatorias son fundamentales en probabilidad y estadística, ya que nos permiten modelar situaciones en las que los resultados no son deterministas. Una variable aleatoria puede tomar diferentes valores, cada uno con una cierta probabilidad.\n",
    "\n",
    "#### 1.3.2.1. Distribuciones discretas y continuas\n",
    "\n",
    "Las distribuciones de probabilidad describen cómo se distribuyen los valores de una variable aleatoria. Estas pueden ser discretas o continuas.\n",
    "\n",
    "##### 1.3.2.1.1. Distribuciones Discretas: Bernoulli, Binomial y Poisson\n",
    "\n",
    "1. **Distribución de Bernoulli:**\n",
    "\n",
    "   - Describe una variable aleatoria que tiene dos posibles resultados: éxito (1) y fracaso (0).\n",
    "   - Ejemplo: Lanzar una moneda donde éxito es cara y fracaso es cruz.\n",
    "   - **Función de probabilidad:** $ P(X = 1) = p $ y $ P(X = 0) = 1 - p $\n",
    "\n",
    "   **Ejemplo práctico en Python:**\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   # Simular 10 lanzamientos de una moneda con probabilidad de éxito p = 0.5\n",
    "   p = 0.5\n",
    "   n = 10\n",
    "   resultados = np.random.binomial(1, p, n)\n",
    "   print(\"Resultados de la simulación de Bernoulli:\", resultados)\n",
    "   ```\n",
    "\n",
    "2. **Distribución Binomial:**\n",
    "\n",
    "   - Describe el número de éxitos en una secuencia de n ensayos de Bernoulli independientes.\n",
    "   - Ejemplo: Contar el número de caras en 10 lanzamientos de una moneda.\n",
    "   - **Función de probabilidad:** $ P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k} $\n",
    "\n",
    "   **Ejemplo práctico en Python:**\n",
    "\n",
    "   ```python\n",
    "   from scipy.stats import binom\n",
    "\n",
    "   # Parámetros de la distribución binomial\n",
    "   n = 10  # número de ensayos\n",
    "   p = 0.5  # probabilidad de éxito en cada ensayo\n",
    "\n",
    "   # Calcular la probabilidad de obtener exactamente 5 caras\n",
    "   k = 5\n",
    "   probabilidad = binom.pmf(k, n, p)\n",
    "   print(f\"Probabilidad de obtener exactamente {k} caras en {n} lanzamientos:\", probabilidad)\n",
    "   ```\n",
    "\n",
    "3. **Distribución de Poisson:**\n",
    "\n",
    "   - Describe el número de eventos que ocurren en un intervalo de tiempo o espacio fijo.\n",
    "   - Ejemplo: Número de llamadas recibidas por una central telefónica en una hora.\n",
    "   - **Función de probabilidad:** $ P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} $\n",
    "\n",
    "   **Ejemplo práctico en Python:**\n",
    "\n",
    "   ```python\n",
    "   from scipy.stats import poisson\n",
    "\n",
    "   # Parámetro de la distribución de Poisson\n",
    "   lam = 3  # número medio de llamadas por hora\n",
    "\n",
    "   # Calcular la probabilidad de recibir exactamente 5 llamadas en una hora\n",
    "   k = 5\n",
    "   probabilidad = poisson.pmf(k, lam)\n",
    "   print(f\"Probabilidad de recibir exactamente {k} llamadas en una hora:\", probabilidad)\n",
    "   ```\n",
    "\n",
    "##### 1.3.2.1.2. Distribuciones Continuas: Normal, Exponencial y t-Student\n",
    "\n",
    "1. **Distribución Normal:**\n",
    "\n",
    "   - Describe una variable aleatoria que sigue una distribución en forma de campana.\n",
    "   - Ejemplo: Altura de personas en una población.\n",
    "   - **Función de densidad de probabilidad:** $ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} $\n",
    "\n",
    "   **Ejemplo práctico en Python:**\n",
    "\n",
    "   ```python\n",
    "   from scipy.stats import norm\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   # Parámetros de la distribución normal\n",
    "   mu, sigma = 0, 1\n",
    "\n",
    "   # Generar datos y graficar\n",
    "   x = np.linspace(-3*sigma, 3*sigma, 100)\n",
    "   plt.plot(x, norm.pdf(x, mu, sigma))\n",
    "   plt.title('Distribución Normal')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "2. **Distribución Exponencial:**\n",
    "\n",
    "   - Describe el tiempo entre eventos en un proceso de Poisson.\n",
    "   - Ejemplo: Tiempo entre llegadas de clientes a una tienda.\n",
    "   - **Función de densidad de probabilidad:** $ f(x) = \\lambda e^{-\\lambda x} $\n",
    "\n",
    "   **Ejemplo práctico en Python:**\n",
    "\n",
    "   ```python\n",
    "   from scipy.stats import expon\n",
    "\n",
    "   # Parámetro de la distribución exponencial\n",
    "   lam = 1\n",
    "\n",
    "   # Generar datos y graficar\n",
    "   x = np.linspace(0, 5, 100)\n",
    "   plt.plot(x, expon.pdf(x, scale=1/lam))\n",
    "   plt.title('Distribución Exponencial')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "3. **Distribución t-Student:**\n",
    "\n",
    "   - Describe una variable aleatoria que se utiliza cuando el tamaño de la muestra es pequeño y/o la varianza poblacional es desconocida.\n",
    "   - Ejemplo: Estimación del promedio de una población con muestra pequeña.\n",
    "   - **Función de densidad de probabilidad:** Más compleja, depende de los grados de libertad.\n",
    "\n",
    "   **Ejemplo práctico en Python:**\n",
    "\n",
    "   ```python\n",
    "   from scipy.stats import t\n",
    "\n",
    "   # Parámetro de la distribución t-Student\n",
    "   df = 10  # grados de libertad\n",
    "\n",
    "   # Generar datos y graficar\n",
    "   x = np.linspace(-4, 4, 100)\n",
    "   plt.plot(x, t.pdf(x, df))\n",
    "   plt.title('Distribución t-Student')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "#### 1.3.2.2. Esperanza y varianza\n",
    "\n",
    "La esperanza (o valor esperado) y la varianza son medidas fundamentales para describir las distribuciones de variables aleatorias.\n",
    "\n",
    "**Esperanza (Valor Esperado):**\n",
    "\n",
    "La esperanza de una variable aleatoria $X$ es una medida del centro de su distribución. Para una variable discreta, se calcula como:\n",
    "\n",
    "$$ E(X) = \\sum_x x \\cdot P(X=x) $$\n",
    "\n",
    "Para una variable continua, se calcula como:\n",
    "\n",
    "$$ E(X) = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx $$\n",
    "\n",
    "**Varianza:**\n",
    "\n",
    "La varianza mide la dispersión de los valores de la variable alrededor de la esperanza. Para una variable discreta, se calcula como:\n",
    "\n",
    "$$ \\text{Var}(X) = E((X - E(X))^2) = \\sum_x (x - E(X))^2 \\cdot P(X=x) $$\n",
    "\n",
    "Para una variable continua, se calcula como:\n",
    "\n",
    "$$ \\text{Var}(X) = \\int_{-\\infty}^{\\infty} (x - E(X))^2 \\cdot f(x) \\, dx $$\n",
    "\n",
    "##### 1.3.2.2.1. Propiedades de la esperanza y varianza\n",
    "\n",
    "1. **Linealidad de la Esperanza:**\n",
    "\n",
    "$$ E(aX + b) = aE(X) + b $$\n",
    "\n",
    "2. **Varianza de una suma de variables independientes:**\n",
    "\n",
    "$$ \\text{Var}(aX + b) = a^2 \\text{Var}(X) $$\n",
    "\n",
    "##### 1.3.2.2.2. Covarianza y correlación\n",
    "\n",
    "**Covarianza:**\n",
    "\n",
    "La covarianza mide la relación lineal entre dos variables aleatorias. Si $X$ y $Y$ son dos variables aleatorias, su covarianza se calcula como:\n",
    "\n",
    "$$ \\text{Cov}(X, Y) = E((X - E(X))(Y - E(Y))) $$\n",
    "\n",
    "**Correlación:**\n",
    "\n",
    "La correlación es una versión normalizada de la covarianza que mide la fuerza y la dirección de la relación lineal entre dos variables. Se calcula como:\n",
    "\n",
    "$$ \\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X) \\cdot \\text{Var}(Y)}} $$\n",
    "\n",
    "**Ejemplo práctico en Python:**\n",
    "\n",
    "Supongamos que tenemos dos variables aleatorias, las alturas y pesos de un grupo de personas, y queremos calcular la covarianza y correlación entre ellas.\n",
    "\n",
    "```python\n",
    "# Datos de ejemplo\n",
    "alturas = np.array([1.60, 1.65, 1.70, 1.75, 1.80])\n",
    "pesos = np.array([60, 65, 70, 75, 80])\n",
    "\n",
    "# Calcular covarianza\n",
    "covarianza = np.cov(alturas, pesos)[0, 1]\n",
    "print(\"Covarianza entre alturas y pesos:\", covarianza)\n",
    "\n",
    "# Calcular correlación\n",
    "correlacion = np.corrcoef(alturas, pesos)[0, 1]\n",
    "print(\"Correlación entre alturas y pesos:\", correlacion)\n",
    "```\n",
    "\n",
    "**Salida:**\n",
    "\n",
    "```\n",
    "Covarianza entre alturas y pesos: 12.5\n",
    "Correlación entre alturas y pesos: 1.0\n",
    "```\n",
    "\n",
    "Estas herramientas y conceptos de probabilidad son esenciales para comprender y modelar los datos en Data Science y Machine Learning. Con ellos, podemos hacer predicciones informadas, evaluar modelos y tomar decisiones basadas en datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
