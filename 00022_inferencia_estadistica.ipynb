{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Inferencia Estadística\n",
    "\n",
    "La inferencia estadística se refiere a los métodos y procedimientos que permiten sacar conclusiones sobre una población a partir de una muestra de datos. Esta rama de la estadística es fundamental en Data Science, ya que nos permite hacer predicciones y tomar decisiones basadas en datos.\n",
    "\n",
    "### 2.2.1. Estimación\n",
    "\n",
    "La estimación es el proceso de inferir el valor de un parámetro poblacional a partir de una muestra. Los métodos de estimación se dividen en estimadores puntuales y por intervalos.\n",
    "\n",
    "#### 2.2.1.1. Estimadores puntuales y por intervalos\n",
    "\n",
    "**Estimadores puntuales:** Un estimador puntual es un valor único que sirve como mejor estimación del parámetro poblacional. Por ejemplo, la media muestral es un estimador puntual de la media poblacional.\n",
    "\n",
    "**Estimadores por intervalos:** Un estimador por intervalos proporciona un rango de valores que, con cierta probabilidad, contiene el parámetro poblacional. Este rango se denomina intervalo de confianza.\n",
    "\n",
    "##### 2.2.1.1.1. Propiedades de los estimadores (sesgo, consistencia)\n",
    "\n",
    "- **Sesgo:** Un estimador es insesgado si su valor esperado es igual al verdadero valor del parámetro. En otras palabras, no tiende a sobrestimar o subestimar el parámetro.\n",
    "  \n",
    "  \\[ \\text{Sesgo}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta \\]\n",
    "\n",
    "- **Consistencia:** Un estimador es consistente si, a medida que aumenta el tamaño de la muestra, el estimador converge en probabilidad al verdadero valor del parámetro.\n",
    "\n",
    "##### 2.2.1.1.2. Intervalos de confianza\n",
    "\n",
    "Un intervalo de confianza proporciona un rango de valores dentro del cual se espera que se encuentre el parámetro poblacional con una cierta probabilidad (nivel de confianza). Por ejemplo, un intervalo de confianza del 95% para la media poblacional puede ser calculado como:\n",
    "\n",
    "\\[ \\bar{x} \\pm z_{\\alpha/2} \\left(\\frac{\\sigma}{\\sqrt{n}}\\right) \\]\n",
    "\n",
    "Donde:\n",
    "- \\(\\bar{x}\\) es la media muestral.\n",
    "- \\(z_{\\alpha/2}\\) es el valor crítico de la distribución normal estándar.\n",
    "- \\(\\sigma\\) es la desviación estándar poblacional.\n",
    "- \\(n\\) es el tamaño de la muestra.\n",
    "\n",
    "#### 2.2.1.2. Métodos de máxima verosimilitud\n",
    "\n",
    "La máxima verosimilitud es un método de estimación que encuentra los valores de los parámetros que maximizan la probabilidad (verosimilitud) de observar los datos dados.\n",
    "\n",
    "##### 2.2.1.2.1. Función de verosimilitud\n",
    "\n",
    "La función de verosimilitud, \\(L(\\theta)\\), es la probabilidad de observar los datos dados los parámetros \\(\\theta\\). Para una muestra de datos independientes y distribuidos idénticamente, la función de verosimilitud es:\n",
    "\n",
    "\\[ L(\\theta) = \\prod_{i=1}^{n} f(x_i; \\theta) \\]\n",
    "\n",
    "Donde \\(f(x_i; \\theta)\\) es la función de densidad de probabilidad de los datos.\n",
    "\n",
    "##### 2.2.1.2.2. Estimación de parámetros con Python\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Datos de ejemplo\n",
    "data = np.random.normal(loc=5, scale=2, size=100)\n",
    "\n",
    "# Función de verosimilitud para una distribución normal\n",
    "def log_likelihood(params, data):\n",
    "    mu, sigma = params\n",
    "    return -np.sum(norm.logpdf(data, mu, sigma))\n",
    "\n",
    "# Estimación de parámetros usando optimización\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "initial_params = [0, 1]  # Inicialización de mu y sigma\n",
    "results = minimize(log_likelihood, initial_params, args=(data,))\n",
    "mu_mle, sigma_mle = results.x\n",
    "\n",
    "print(f\"Estimación de máxima verosimilitud: mu = {mu_mle}, sigma = {sigma_mle}\")\n",
    "```\n",
    "\n",
    "### 2.2.2. Pruebas de hipótesis\n",
    "\n",
    "Las pruebas de hipótesis son procedimientos estadísticos que permiten tomar decisiones sobre los parámetros poblacionales basándose en los datos muestrales. Estas pruebas ayudan a validar o refutar una hipótesis formulada.\n",
    "\n",
    "#### 2.2.2.1. Pruebas paramétricas y no paramétricas\n",
    "\n",
    "**Pruebas paramétricas:** Estas pruebas asumen que los datos siguen una distribución específica. Ejemplos incluyen el t-test y ANOVA.\n",
    "\n",
    "**Pruebas no paramétricas:** Estas pruebas no asumen una distribución específica para los datos. Ejemplos incluyen la prueba de Chi-cuadrado y la prueba de Wilcoxon.\n",
    "\n",
    "##### 2.2.2.1.1. t-test y ANOVA\n",
    "\n",
    "- **t-test:** Se utiliza para comparar las medias de dos grupos. Puede ser de una muestra (para comparar la media muestral con un valor conocido) o de dos muestras independientes.\n",
    "\n",
    "  \\[ t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{s_p^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}} \\]\n",
    "\n",
    "  Donde \\(s_p^2\\) es la varianza combinada de las dos muestras.\n",
    "\n",
    "- **ANOVA:** Se utiliza para comparar las medias de tres o más grupos. Analiza las diferencias entre las medias y determina si al menos una de ellas es significativamente diferente.\n",
    "\n",
    "  \\[ F = \\frac{\\text{Variabilidad entre grupos}}{\\text{Variabilidad dentro de los grupos}} \\]\n",
    "\n",
    "##### 2.2.2.1.2. Chi-cuadrado y prueba de Wilcoxon\n",
    "\n",
    "- **Chi-cuadrado:** Se utiliza para evaluar la independencia entre dos variables categóricas.\n",
    "\n",
    "  \\[ \\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i} \\]\n",
    "\n",
    "  Donde \\(O_i\\) son las frecuencias observadas y \\(E_i\\) son las frecuencias esperadas.\n",
    "\n",
    "- **Prueba de Wilcoxon:** Es una prueba no paramétrica que compara dos muestras relacionadas o emparejadas. \n",
    "\n",
    "#### 2.2.2.2. Implementación de pruebas en Python\n",
    "\n",
    "Python proporciona herramientas en `scipy.stats` para realizar estas pruebas de manera sencilla.\n",
    "\n",
    "##### 2.2.2.2.1. Uso de scipy.stats\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# Datos de ejemplo\n",
    "data1 = np.random.normal(loc=5, scale=2, size=100)\n",
    "data2 = np.random.normal(loc=6, scale=2, size=100)\n",
    "\n",
    "# t-test para muestras independientes\n",
    "t_stat, p_value = stats.ttest_ind(data1, data2)\n",
    "print(f\"t-test: t_stat = {t_stat}, p_value = {p_value}\")\n",
    "\n",
    "# ANOVA para tres grupos\n",
    "data3 = np.random.normal(loc=7, scale=2, size=100)\n",
    "f_stat, p_value = stats.f_oneway(data1, data2, data3)\n",
    "print(f\"ANOVA: F_stat = {f_stat}, p_value = {p_value}\")\n",
    "\n",
    "# Prueba de Chi-cuadrado\n",
    "observed = np.array([10, 20, 30])\n",
    "expected = np.array([15, 15, 30])\n",
    "chi2_stat, p_value = stats.chisquare(observed, expected)\n",
    "print(f\"Chi-cuadrado: chi2_stat = {chi2_stat}, p_value = {p_value}\")\n",
    "\n",
    "# Prueba de Wilcoxon\n",
    "data4 = np.random.normal(loc=5, scale=2, size=100)\n",
    "wilcoxon_stat, p_value = stats.wilcoxon(data1, data4)\n",
    "print(f\"Prueba de Wilcoxon: stat = {wilcoxon_stat}, p_value = {p_value}\")\n",
    "```\n",
    "\n",
    "##### 2.2.2.2.2. Interpretación y visualización de resultados\n",
    "\n",
    "```python\n",
    "# Visualización de los resultados del t-test\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data1, color='blue', kde=True, label='Grupo 1')\n",
    "sns.histplot(data2, color='orange', kde=True, label='Grupo 2')\n",
    "plt.axvline(np.mean(data1), color='blue', linestyle='--', label='Media Grupo 1')\n",
    "plt.axvline(np.mean(data2), color='orange', linestyle='--', label='Media Grupo 2')\n",
    "plt.legend()\n",
    "plt.title('Comparación de dos grupos con t-test')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "En conclusión, la inferencia estadística nos permite hacer estimaciones y tomar decisiones informadas basadas en datos muestrales. Mediante el uso de estimadores puntuales y por intervalos, y la realización de pruebas de hipótesis, podemos validar o refutar hipótesis y obtener una comprensión más profunda de los datos. La implementación en Python facilita estos análisis, haciendo que la inferencia estadística sea accesible y aplicable en diversos contextos de Data Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
