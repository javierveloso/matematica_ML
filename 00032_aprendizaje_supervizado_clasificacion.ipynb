{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Clasificación\n",
    "\n",
    "La clasificación es una técnica de aprendizaje supervisado utilizada para predecir etiquetas categóricas. A diferencia de la regresión, donde el objetivo es predecir valores continuos, la clasificación se enfoca en asignar datos a categorías discretas. A continuación, exploraremos la regresión logística y los árboles de decisión junto con técnicas avanzadas como bosques aleatorios y métodos de boosting.\n",
    "\n",
    "### 3.2.1. Regresión logística\n",
    "\n",
    "La regresión logística es un modelo utilizado para la clasificación binaria, es decir, cuando la variable de salida tiene dos posibles valores (0 o 1, verdadero o falso, etc.). Se basa en la función sigmoide para mapear cualquier valor real a un rango entre 0 y 1.\n",
    "\n",
    "#### 3.2.1.1. Función sigmoide y máxima verosimilitud\n",
    "\n",
    "**Función sigmoide:** La función sigmoide transforma una entrada lineal en una probabilidad.\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "Donde $z$ es una combinación lineal de las características del modelo.\n",
    "\n",
    "**Máxima verosimilitud:** En la regresión logística, los parámetros se estiman mediante la maximización de la función de verosimilitud.\n",
    "\n",
    "$$ L(\\beta) = \\prod_{i=1}^{n} \\sigma(x_i \\cdot \\beta)^{y_i} (1 - \\sigma(x_i \\cdot \\beta))^{1 - y_i} $$\n",
    "\n",
    "Maximizar esta función equivale a encontrar los mejores coeficientes $\\beta$ que expliquen los datos observados.\n",
    "\n",
    "##### 3.2.1.1.1. Interpretación de coeficientes\n",
    "\n",
    "Cada coeficiente $\\beta_j$ en la regresión logística representa el cambio en el logit (log-odds) por unidad de cambio en la característica $x_j$. Si $\\beta_j$ es positivo, un aumento en $x_j$ incrementa la probabilidad de $y = 1$; si es negativo, la reduce.\n",
    "\n",
    "##### 3.2.1.1.2. Odds ratio y probabilidades\n",
    "\n",
    "El odds ratio es una forma de interpretar los coeficientes en términos de probabilidades.\n",
    "\n",
    "$$ \\text{Odds ratio} = e^{\\beta_j} $$\n",
    "\n",
    "Un odds ratio mayor que 1 indica que la variable $x_j$ está asociada con mayores probabilidades de que $y = 1$.\n",
    "\n",
    "#### 3.2.1.2. Implementación y evaluación en Python\n",
    "\n",
    "##### 3.2.1.2.1. Uso de scikit-learn para clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.35\n",
      "Precisión (Precision): 0.35\n",
      "Sensibilidad (Recall): 1.0\n",
      "F1-Score: 0.5185185185185185\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Datos de ejemplo\n",
    "data = {\n",
    "    'x1': np.random.rand(100),\n",
    "    'x2': np.random.rand(100),\n",
    "    'y': np.random.randint(0, 2, 100)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# División de datos en entrenamiento y prueba\n",
    "X = df[['x1', 'x2']]\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(f\"Precisión: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Precisión (Precision): {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Sensibilidad (Recall): {recall_score(y_test, y_pred)}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1.2.2. Métricas de evaluación (precisión, recall, F1-score)\n",
    "\n",
    "- **Precisión (Accuracy):** Proporción de predicciones correctas sobre el total de predicciones.\n",
    "- **Precisión (Precision):** Proporción de verdaderos positivos sobre el total de positivos predichos.\n",
    "- **Sensibilidad (Recall):** Proporción de verdaderos positivos sobre el total de positivos reales.\n",
    "- **F1-Score:** Media armónica de la precisión y la sensibilidad.\n",
    "\n",
    "### 3.2.2. Árboles de decisión y bosques aleatorios\n",
    "\n",
    "Los árboles de decisión son modelos predictivos que dividen iterativamente los datos en subconjuntos más pequeños basados en características, formando una estructura similar a un árbol.\n",
    "\n",
    "#### 3.2.2.1. Criterios de división y poda\n",
    "\n",
    "**Criterios de división:** Determinan cómo se dividen los nodos en un árbol de decisión.\n",
    "\n",
    "- **Ganancia de información:** Mide la reducción de la incertidumbre (entropía) al dividir los datos.\n",
    "\n",
    "  $$ \\text{Ganancia de información} = \\text{Entropía}(S) - \\sum_{i=1}^{m} \\frac{|S_i|}{|S|} \\text{Entropía}(S_i) $$\n",
    "\n",
    "- **Índice Gini:** Mide la pureza de una división. Un valor de Gini más bajo indica una mayor pureza.\n",
    "\n",
    "  $$ \\text{Índice Gini} = 1 - \\sum_{i=1}^{c} p_i^2 $$\n",
    "\n",
    "##### 3.2.2.1.1. Ganancia de información y índice Gini\n",
    "\n",
    "La ganancia de información y el índice Gini son utilizados para seleccionar las divisiones en los nodos de un árbol de decisión. La ganancia de información se basa en la teoría de la información, mientras que el índice Gini es una medida de pureza.\n",
    "\n",
    "##### 3.2.2.1.2. Técnicas de poda para evitar sobreajuste\n",
    "\n",
    "La poda reduce el tamaño de los árboles de decisión eliminando ramas que no contribuyen significativamente a la predicción, lo que ayuda a prevenir el sobreajuste.\n",
    "\n",
    "- **Poda temprana (Pre-pruning):** Detiene el crecimiento del árbol antes de que se ajuste demasiado a los datos.\n",
    "- **Poda tardía (Post-pruning):** Elimina nodos del árbol después de que se haya creado completamente, basándose en su contribución a la precisión del modelo.\n",
    "\n",
    "#### 3.2.2.2. Bagging y boosting\n",
    "\n",
    "**Bagging (Bootstrap Aggregating):** Combina múltiples modelos entrenados en diferentes subconjuntos de datos para reducir la varianza y mejorar la precisión. Un ejemplo es Random Forest.\n",
    "\n",
    "**Boosting:** Combina modelos secuenciales donde cada modelo intenta corregir los errores del anterior. Un ejemplo es Gradient Boosting.\n",
    "\n",
    "##### 3.2.2.2.1. Random Forests y su implementación\n",
    "\n",
    "Random Forests es un algoritmo de bagging que construye múltiples árboles de decisión y promedia sus predicciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (Random Forest): 0.65\n",
      "Precisión (Precision - RF): 0.5\n",
      "Sensibilidad (Recall - RF): 0.8571428571428571\n",
      "F1-Score (RF): 0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(f\"Precisión (Random Forest): {accuracy_score(y_test, y_pred_rf)}\")\n",
    "print(f\"Precisión (Precision - RF): {precision_score(y_test, y_pred_rf)}\")\n",
    "print(f\"Sensibilidad (Recall - RF): {recall_score(y_test, y_pred_rf)}\")\n",
    "print(f\"F1-Score (RF): {f1_score(y_test, y_pred_rf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2.2.2. Gradient Boosting y XGBoost\n",
    "\n",
    "**Gradient Boosting:** Construye modelos secuenciales donde cada modelo intenta corregir los errores del anterior utilizando gradientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (Gradient Boosting): 0.45\n",
      "Precisión (Precision - GB): 0.375\n",
      "Sensibilidad (Recall - GB): 0.8571428571428571\n",
      "F1-Score (GB): 0.5217391304347826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Entrenamiento del modelo Gradient Boosting\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=0)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicción\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(f\"Precisión (Gradient Boosting): {accuracy_score(y_test, y_pred_gb)}\")\n",
    "print(f\"Precisión (Precision - GB): {precision_score(y_test, y_pred_gb)}\")\n",
    "print(f\"Sensibilidad (Recall - GB): {recall_score(y_test, y_pred_gb)}\")\n",
    "print(f\"F1-Score (GB): {f1_score(y_test, y_pred_gb)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost:** Una implementación optimizada de Gradient Boosting que mejora la velocidad y el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (XGBoost): 0.65\n",
      "Precisión (Precision - XGBoost): 0.5\n",
      "Sensibilidad (Recall - XGBoost): 0.8571428571428571\n",
      "F1-Score (XGBoost): 0.631578947368421\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Datos de ejemplo para XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Parámetros del modelo\n",
    "params = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'binary:logistic'\n",
    "}\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "num_round = 100\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Predicción\n",
    "y_pred_xgb = bst.predict(dtest)\n",
    "y_pred_xgb = np.round(y_pred_xgb)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(f\"Precisión (XGBoost): {accuracy_score(y_test, y_pred_xgb)}\")\n",
    "print(f\"Precisión (Precision - XGBoost): {precision_score(y_test, y_pred_xgb)}\")\n",
    "print(f\"Sensibilidad (Recall - XGBoost): {recall_score(y_test, y_pred_xgb)}\")\n",
    "print(f\"F1-Score (XGBoost): {f1_score(y_test, y_pred_xgb)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
